{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["DnhBtnVJByN1","Imo8nPpzB6_4","wNz1lJFoaU11","L7LNejIaCHnC","yWr9Wbfy1QbJ","-2hV8fiBPHgb"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"86gRL0k9tUh5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687933413225,"user_tz":-180,"elapsed":44144,"user":{"displayName":"Mohannad Barakat","userId":"02596848204094181562"}},"outputId":"2ad47fa2-e4a7-4e7e-fc80-e7a59adfabe9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.40.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Collecting git+https://github.com/Noha-Magdy/segment-anything.git\n","  Cloning https://github.com/Noha-Magdy/segment-anything.git to /tmp/pip-req-build-00cm2msj\n","  Running command git clone --filter=blob:none --quiet https://github.com/Noha-Magdy/segment-anything.git /tmp/pip-req-build-00cm2msj\n","  Resolved https://github.com/Noha-Magdy/segment-anything.git to commit 98439a88601f4290b63152ab48b1efc88e4c6cd9\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: segment-anything\n","  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36606 sha256=8a7e64218d8d178f65cb799d19a097841d2b227e2ef09d1e56939ca312b53174\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-4mftp2nl/wheels/a5/4a/ea/b9ae92897766fc687a9f74ac9c0351adfd61e2df1e37cb2f16\n","Successfully built segment-anything\n","Installing collected packages: segment-anything\n","Successfully installed segment-anything-1.0\n","--2023-06-28 06:23:07--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.8.51, 13.35.8.29, 13.35.8.19, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.8.51|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2564550879 (2.4G) [binary/octet-stream]\n","Saving to: ‘sam_vit_h_4b8939.pth’\n","\n","sam_vit_h_4b8939.pt 100%[===================>]   2.39G   191MB/s    in 18s     \n","\n","2023-06-28 06:23:26 (133 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n","\n","Collecting monai\n","  Downloading monai-1.2.0-202306081546-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.0.1+cu118)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from monai) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.6.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->monai) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->monai) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai) (1.3.0)\n","Installing collected packages: monai\n","Successfully installed monai-1.2.0\n"]}],"source":["import sys\n","!{sys.executable} -m pip install opencv-python matplotlib\n","!{sys.executable} -m pip install 'git+https://github.com/Noha-Magdy/segment-anything.git'\n","\n","! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n","!pip install monai"]},{"cell_type":"markdown","source":["## Pipline for saved 2d data"],"metadata":{"id":"DnhBtnVJByN1"}},{"cell_type":"code","source":["# saved 2d data\n","\n","print(\"No bounding box, flair\")\n","import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchvision.transforms import Compose, ToTensor, Resize\n","from torch.utils.data import Subset\n","\n","from tqdm import tqdm\n","from torchsummary import summary\n","import monai\n","from segment_anything import SamPredictor, sam_model_registry\n","from segment_anything.utils.transforms import ResizeLongestSide\n","# import wandb\n","import nibabel as nib\n","from segment_anything.utils.transforms import ResizeLongestSide\n","import torch.nn.functional as F\n","from skimage import transform, io, segmentation\n","\n","\n","torch.manual_seed(2023)\n","np.random.seed(2023)\n","\n","# change here\n","data_path = \"/content/out\"\n","\n","\n","def squarify(M,val = 0):\n","    (a,b)=M.shape\n","    if a>b:\n","        padding=((0,0),(0,a-b))\n","    else:\n","        padding=((0,b-a),(0,0))\n","    return np.pad(M,padding,mode='constant',constant_values=val)\n","\n","\n","\n","def get_bounding_box(mask):\n","  x2 = mask.shape[0]\n","  y2 = mask.shape[1]\n","\n","  return [0, 0, x2, y2]\n","\n","\n","def dice_loss(y_true, y_pred):\n","    \"\"\"\n","    implementation for generalized dice score loss\n","    GDL = 1 - (2 * sum(intersection)/(sum(pred)+sum(true)))\n","    \"\"\"\n","\n","    # y_pred = tf.round(y_pred)\n","    intersection = torch.sum(y_true * y_pred)\n","    pred_sum = torch.sum( y_pred)\n","    true_sum = torch.sum( y_true)\n","    smooth = torch.ones_like(intersection)*1e-5\n","    return 1-((2*intersection+smooth)/(pred_sum+true_sum+smooth))\n","\n","\n","model_type = 'vit_h'\n","checkpoint = 'sam_vit_h_4b8939.pth'\n","sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to('cuda')\n","\n","\n","\n","embedding_path = \"/content/BraTS_data/flair/z/embeddings\"\n","ground_truth_path = \"/content/BraTS_data/mask/z\"\n","\n","train_list = os.listdir(embedding_path)\n","train_list = train_list[:int(len(train_list)*0.7)]\n","val_list = train_list[int(len(train_list)*0.7):]\n","\n","class BraTSDataset(Dataset):\n","\n","    def __init__(self, ground_truth_path, embedding_path, list_of_data, device='cuda'):\n","        \"\"\"\n","        Args:\n","            embedding_path (string): Path to  images.\n","            masks_path (string): path to masks.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.ground_truth_path = ground_truth_path\n","        self.device = device\n","        self.list_of_data = list_of_data\n","        self.embedding_path = embedding_path\n","\n","    def __len__(self):\n","        return len(self.list_of_data)\n","\n","    def __getitem__(self, idx):\n","\n","\n","      embedding = np.load(self.embedding_path + \"/\" + self.list_of_data[idx])\n","      embedding = torch.as_tensor(embedding, dtype=torch.float, device=self.device)\n","\n","\n","      #--------------------------------------------------\n","\n","      mask = np.load(self.ground_truth_path + \"/\" + self.list_of_data[idx])\n","      mask[mask>0] = 1\n","      box = get_bounding_box(mask)\n","      # mask = squarify(mask)\n","\n","\n","\n","      mask = transform.resize(\n","                      mask,\n","                      (256, 256),\n","                      order=0,\n","                      preserve_range=True,\n","                      mode=\"constant\",\n","                  )\n","\n","\n","        # class_1 = np.zeros_like(mask)\n","        # class_2 = np.zeros_like(mask)\n","        # class_3 = np.zeros_like(mask)\n","\n","        # class_1[np.where(mask == 1)] = 1\n","        # class_2[np.where(mask == 2)] = 1\n","        # class_3[np.where(mask == 4)] = 1\n","\n","        # mask = mask = np.stack((class_1, class_2, class_3), axis = -1 )\n","      mask = torch.as_tensor(mask).to(self.device)\n","      mask = mask[None,...]\n","\n","\n","      box = get_bounding_box(mask)\n","      # box = [0,0,mask.shape[-2],mask.shape[-1]]\n","      box_np = np.array(box)\n","      sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)\n","      box = sam_trans.apply_boxes(box_np, (mask.shape[-2], mask.shape[-1]))\n","      box_torch = torch.as_tensor(box, dtype=torch.float, device=self.device)\n","\n","\n","      sample = {\"embedding\": embedding,\n","                  'mask': mask,\n","                  'box' : box_torch\n","                  }\n","\n","      return sample\n","\n","\n","train = BraTSDataset(ground_truth_path, embedding_path, train_list)\n","val = BraTSDataset(ground_truth_path, embedding_path, val_list)\n","\n","\n","\n","train_loader = DataLoader(train, batch_size= 8, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val, batch_size= 4, shuffle=True, num_workers=0)\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","hprams = {\n","    \"num_epochs\": 15,\n","    \"best_loss\": 1e10,\n","    \"model_save_path\": \"/content\",\n","    \"lr\":1e-5,\n","    \"weight_decay\":0,\n","    \"device\": device,\n","    \"train_dataloader\": train_loader,\n","    \"val_dataloader\": val_loader\n","    }\n","\n","def eval(val_dataloader, sam_model, seg_loss):\n","  loss = 0\n","  for step_val, data in enumerate(val_dataloader):\n","    embedding, mask, box = data[\"embedding\"], data[\"mask\"], data[\"box\"]\n","\n","    with torch.no_grad():\n","      # embedding = sam_model.image_encoder(image)\n","      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","          points=None,\n","          boxes=box,\n","          masks=None,\n","      )\n","\n","      # predicted masks\n","      mask_predictions, _ = sam_model.mask_decoder(\n","      image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","      image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","      sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","      dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","      multimask_output=False,\n","      )\n","\n","      loss += seg_loss(mask_predictions, mask.to(device))\n","  loss /= step_val+1\n","  return loss\n","\n","\n","\n","def train(**kwargs):\n","    model_save_path = kwargs['model_save_path']\n","    num_epochs = kwargs['num_epochs']\n","    best_loss = kwargs['best_loss']\n","    lr = kwargs['lr']\n","    weight_decay = kwargs['weight_decay']\n","    device = kwargs['device']\n","    train_dataloader = kwargs['train_dataloader']\n","    # embedding_path = kwargs['embedding_path']\n","    val_dataloader = kwargs['val_dataloader']\n","\n","    os.makedirs(model_save_path, exist_ok=True)\n","\n","    sam_model.train()\n","    optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, factor=0.25,  patience=5)\n","    seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n","\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        # train\n","        for step, data in enumerate(tqdm(train_dataloader)):\n","            mask, box, embedding =  data[\"mask\"], data[\"box\"], data[\"embedding\"]\n","            embedding.to(device)\n","\n","            with torch.no_grad():\n","\n","              sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","                  points=None,\n","                  boxes=box,\n","                  masks=None,\n","              )\n","\n","            # predicted masks\n","            mask_predictions, _ = sam_model.mask_decoder(\n","              image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","              image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","              sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","              dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","              multimask_output=False,\n","            )\n","\n","            loss = seg_loss(mask_predictions, mask.to(device))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","            # break\n","        epoch_loss /= step+1\n","        scheduler.step(epoch_loss)\n","        val_loss = None\n","        if val_dataloader is not None: val_loss = eval(val_dataloader, sam_model, seg_loss)\n","        print(f'EPOCH: {epoch}, Loss: {epoch_loss}, Val loss {val_loss}')\n","        # wandb.log({\"epoch\": epoch, \"loss\": loss})\n","\n","        # save the latest model checkpoint\n","        # torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_'+str(epoch)+'.pth'))\n","\n","        # save the best model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_best.pth'))\n","\n","\n","\n","\n","train(**hprams)"],"metadata":{"id":"ENt59cGUt0Py","colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"status":"error","timestamp":1687933443234,"user_tz":-180,"elapsed":30013,"user":{"displayName":"Mohannad Barakat","userId":"02596848204094181562"}},"outputId":"c5849cea-af63-47d5-f0d1-a9609c8be187"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["No bounding box, flair\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-1031f46d13de>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mground_truth_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/BraTS_data/mask/z\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mtrain_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0mtrain_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mval_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/BraTS_data/flair/z/embeddings'"]}]},{"cell_type":"markdown","source":["## pipeline for saved 2d embeddings and 3d masks (no bounding box)"],"metadata":{"id":"Imo8nPpzB6_4"}},{"cell_type":"code","source":["# 2d input 3d mask\n","\n","print(\"No bounding box, flair_z\")\n","import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchvision.transforms import Compose, ToTensor, Resize\n","from torch.utils.data import Subset\n","\n","from tqdm import tqdm\n","from torchsummary import summary\n","import monai\n","from segment_anything import SamPredictor, sam_model_registry\n","from segment_anything.utils.transforms import ResizeLongestSide\n","# import wandb\n","import nibabel as nib\n","from segment_anything.utils.transforms import ResizeLongestSide\n","import torch.nn.functional as F\n","from skimage import transform, io, segmentation\n","\n","\n","torch.manual_seed(2023)\n","np.random.seed(2023)\n","\n","# change here\n","data_path = \"/content/out\"\n","\n","\n","def squarify(M,val = 0):\n","    (a,b)=M.shape\n","    if a>b:\n","        padding=((0,0),(0,a-b))\n","    else:\n","        padding=((0,b-a),(0,0))\n","    return np.pad(M,padding,mode='constant',constant_values=val)\n","\n","\n","\n","def get_bounding_box(mask):\n","  x2 = mask.shape[0]\n","  y2 = mask.shape[1]\n","\n","  return [0, 0, x2, y2]\n","\n","\n","def dice_loss(y_true, y_pred):\n","    \"\"\"\n","    implementation for generalized dice score loss\n","    GDL = 1 - (2 * sum(intersection)/(sum(pred)+sum(true)))\n","    \"\"\"\n","\n","    # y_pred = tf.round(y_pred)\n","    intersection = torch.sum(y_true * y_pred)\n","    pred_sum = torch.sum( y_pred)\n","    true_sum = torch.sum( y_true)\n","    smooth = torch.ones_like(intersection)*1e-5\n","    return 1-((2*intersection+smooth)/(pred_sum+true_sum+smooth))\n","\n","\n","model_type = 'vit_h'\n","checkpoint = 'sam_vit_h_4b8939.pth'\n","sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to('cuda')\n","\n","\n","\n","embedding_path = \"/content/BraTS_data/flair/z/embeddings\"\n","ground_truth_path = \"/content/out\"\n","\n","train_list = os.listdir(embedding_path)\n","train_list = train_list[:int(len(train_list)*0.7)]\n","val_list = train_list[int(len(train_list)*0.7):]\n","\n","class BraTSDataset(Dataset):\n","\n","    def __init__(self, ground_truth_path, embedding_path, list_of_data, device='cuda'):\n","        \"\"\"\n","        Args:\n","            embedding_path (string): Path to  images.\n","            masks_path (string): path to masks.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.ground_truth_path = ground_truth_path\n","        self.device = device\n","        self.list_of_data = list_of_data\n","        self.embedding_path = embedding_path\n","\n","    def __len__(self):\n","        return len(self.list_of_data)\n","\n","    def __getitem__(self, idx):\n","\n","\n","      embedding = np.load(self.embedding_path + \"/\" + self.list_of_data[idx])\n","      embedding = torch.as_tensor(embedding, dtype=torch.float, device=self.device)\n","\n","\n","      #--------------------------------------------------\n","      case_id = self.list_of_data[idx].split(\"_\")[1]\n","      slice_ = int(self.list_of_data[idx].split(\"_\")[-1].split(\".\")[0])\n","      # \"BraTS2021_00000\"\n","      case_name = \"BraTS2021_\"+ case_id\n","      img =  nib.load(self.ground_truth_path + \"/\" + case_name + \"/\"+ case_name +\"_seg.nii.gz\")\n","      img = img.get_fdata()\n","      mask = img[:,:,slice_]\n","\n","      mask[mask>0] = 1\n","      box = get_bounding_box(mask)\n","      # mask = squarify(mask)\n","\n","\n","\n","      mask = transform.resize(\n","                      mask,\n","                      (256, 256),\n","                      order=0,\n","                      preserve_range=True,\n","                      mode=\"constant\",\n","                  )\n","\n","\n","        # class_1 = np.zeros_like(mask)\n","        # class_2 = np.zeros_like(mask)\n","        # class_3 = np.zeros_like(mask)\n","\n","        # class_1[np.where(mask == 1)] = 1\n","        # class_2[np.where(mask == 2)] = 1\n","        # class_3[np.where(mask == 4)] = 1\n","\n","        # mask = mask = np.stack((class_1, class_2, class_3), axis = -1 )\n","      mask = torch.as_tensor(mask).to(self.device)\n","      mask = mask[None,...]\n","\n","\n","      box = get_bounding_box(mask)\n","      # box = [0,0,mask.shape[-2],mask.shape[-1]]\n","      box_np = np.array(box)\n","      sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)\n","      box = sam_trans.apply_boxes(box_np, (mask.shape[-2], mask.shape[-1]))\n","      box_torch = torch.as_tensor(box, dtype=torch.float, device=self.device)\n","\n","\n","      sample = {\"embedding\": embedding,\n","                  'mask': mask,\n","                  'box' : box_torch\n","                  }\n","\n","      return sample\n","\n","\n","train = BraTSDataset(ground_truth_path, embedding_path, train_list)\n","val = BraTSDataset(ground_truth_path, embedding_path, val_list)\n","\n","\n","\n","train_loader = DataLoader(train, batch_size= 8, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val, batch_size= 4, shuffle=True, num_workers=0)\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","hprams = {\n","    \"num_epochs\": 15,\n","    \"best_loss\": 1e10,\n","    \"model_save_path\": \"/content\",\n","    \"lr\":1e-5,\n","    \"weight_decay\":0,\n","    \"device\": device,\n","    \"train_dataloader\": train_loader,\n","    \"val_dataloader\": val_loader\n","    }\n","\n","def eval(val_dataloader, sam_model, seg_loss):\n","  loss = 0\n","  for step_val, data in enumerate(val_dataloader):\n","    embedding, mask, box = data[\"embedding\"], data[\"mask\"], data[\"box\"]\n","\n","    with torch.no_grad():\n","      # embedding = sam_model.image_encoder(image)\n","      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","          points=None,\n","          boxes=box,\n","          masks=None,\n","      )\n","\n","      # predicted masks\n","      mask_predictions, _ = sam_model.mask_decoder(\n","      image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","      image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","      sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","      dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","      multimask_output=False,\n","      )\n","\n","      loss += seg_loss(mask_predictions, mask.to(device))\n","  loss /= step_val+1\n","  return loss\n","\n","\n","\n","def train(**kwargs):\n","    model_save_path = kwargs['model_save_path']\n","    num_epochs = kwargs['num_epochs']\n","    best_loss = kwargs['best_loss']\n","    lr = kwargs['lr']\n","    weight_decay = kwargs['weight_decay']\n","    device = kwargs['device']\n","    train_dataloader = kwargs['train_dataloader']\n","    # embedding_path = kwargs['embedding_path']\n","    val_dataloader = kwargs['val_dataloader']\n","\n","    os.makedirs(model_save_path, exist_ok=True)\n","\n","    sam_model.train()\n","    optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, factor=0.25,  patience=5)\n","    seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n","\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        # train\n","        for step, data in enumerate(tqdm(train_dataloader)):\n","            mask, box, embedding =  data[\"mask\"], data[\"box\"], data[\"embedding\"]\n","            embedding.to(device)\n","\n","            with torch.no_grad():\n","\n","              sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","                  points=None,\n","                  boxes=box,\n","                  masks=None,\n","              )\n","\n","            # predicted masks\n","            mask_predictions, _ = sam_model.mask_decoder(\n","              image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","              image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","              sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","              dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","              multimask_output=False,\n","            )\n","\n","            loss = seg_loss(mask_predictions, mask.to(device))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","            # break\n","        epoch_loss /= step+1\n","        scheduler.step(epoch_loss)\n","        val_loss = None\n","        if val_dataloader is not None: val_loss = eval(val_dataloader, sam_model, seg_loss)\n","        print(f'EPOCH: {epoch}, Loss: {epoch_loss}, Val loss {val_loss}')\n","        # wandb.log({\"epoch\": epoch, \"loss\": loss})\n","\n","        # save the latest model checkpoint\n","        # torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_'+str(epoch)+'.pth'))\n","\n","        # save the best model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_best.pth'))\n","\n","\n","\n","\n","train(**hprams)"],"metadata":{"id":"cc7tj6e34dLI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2d input 3d mask\n","\n","print(\"No bounding box, flair_x\")\n","import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchvision.transforms import Compose, ToTensor, Resize\n","from torch.utils.data import Subset\n","\n","from tqdm import tqdm\n","from torchsummary import summary\n","import monai\n","from segment_anything import SamPredictor, sam_model_registry\n","from segment_anything.utils.transforms import ResizeLongestSide\n","# import wandb\n","import nibabel as nib\n","from segment_anything.utils.transforms import ResizeLongestSide\n","import torch.nn.functional as F\n","from skimage import transform, io, segmentation\n","\n","\n","torch.manual_seed(2023)\n","np.random.seed(2023)\n","\n","# change here\n","data_path = \"/content/out\"\n","\n","\n","def squarify(M,val = 0):\n","    (a,b)=M.shape\n","    if a>b:\n","        padding=((0,0),(0,a-b))\n","    else:\n","        padding=((0,b-a),(0,0))\n","    return np.pad(M,padding,mode='constant',constant_values=val)\n","\n","\n","\n","def get_bounding_box(mask):\n","  x2 = mask.shape[0]\n","  y2 = mask.shape[1]\n","\n","  return [0, 0, x2, y2]\n","\n","\n","def dice_loss(y_true, y_pred):\n","    \"\"\"\n","    implementation for generalized dice score loss\n","    GDL = 1 - (2 * sum(intersection)/(sum(pred)+sum(true)))\n","    \"\"\"\n","\n","    # y_pred = tf.round(y_pred)\n","    intersection = torch.sum(y_true * y_pred)\n","    pred_sum = torch.sum( y_pred)\n","    true_sum = torch.sum( y_true)\n","    smooth = torch.ones_like(intersection)*1e-5\n","    return 1-((2*intersection+smooth)/(pred_sum+true_sum+smooth))\n","\n","\n","model_type = 'vit_h'\n","checkpoint = 'sam_vit_h_4b8939.pth'\n","sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to('cuda')\n","\n","\n","\n","embedding_path = \"/content/BraTS_data/flair/z/embeddings\"\n","ground_truth_path = \"/content/out\"\n","\n","train_list = os.listdir(embedding_path)\n","train_list = train_list[:int(len(train_list)*0.7)]\n","val_list = train_list[int(len(train_list)*0.7):]\n","\n","class BraTSDataset(Dataset):\n","\n","    def __init__(self, ground_truth_path, embedding_path, list_of_data, device='cuda'):\n","        \"\"\"\n","        Args:\n","            embedding_path (string): Path to  images.\n","            masks_path (string): path to masks.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.ground_truth_path = ground_truth_path\n","        self.device = device\n","        self.list_of_data = list_of_data\n","        self.embedding_path = embedding_path\n","\n","    def __len__(self):\n","        return len(self.list_of_data)\n","\n","    def __getitem__(self, idx):\n","\n","\n","      embedding = np.load(self.embedding_path + \"/\" + self.list_of_data[idx])\n","      embedding = torch.as_tensor(embedding, dtype=torch.float, device=self.device)\n","\n","\n","      #--------------------------------------------------\n","      case_id = self.list_of_data[idx].split(\"_\")[1]\n","      slice_ = int(self.list_of_data[idx].split(\"_\")[-1].split(\".\")[0])\n","      # \"BraTS2021_00000\"\n","      case_name = \"BraTS2021_\"+ case_id\n","      img =  nib.load(self.ground_truth_path + \"/\" + case_name + \"/\"+ case_name +\"_seg.nii.gz\")\n","      img = img.get_fdata()\n","      mask = img[slice_,:,:]\n","\n","      mask[mask>0] = 1\n","      box = get_bounding_box(mask)\n","      # mask = squarify(mask)\n","\n","\n","\n","      mask = transform.resize(\n","                      mask,\n","                      (256, 256),\n","                      order=0,\n","                      preserve_range=True,\n","                      mode=\"constant\",\n","                  )\n","\n","\n","        # class_1 = np.zeros_like(mask)\n","        # class_2 = np.zeros_like(mask)\n","        # class_3 = np.zeros_like(mask)\n","\n","        # class_1[np.where(mask == 1)] = 1\n","        # class_2[np.where(mask == 2)] = 1\n","        # class_3[np.where(mask == 4)] = 1\n","\n","        # mask = mask = np.stack((class_1, class_2, class_3), axis = -1 )\n","      mask = torch.as_tensor(mask).to(self.device)\n","      mask = mask[None,...]\n","\n","\n","      box = get_bounding_box(mask)\n","      # box = [0,0,mask.shape[-2],mask.shape[-1]]\n","      box_np = np.array(box)\n","      sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)\n","      box = sam_trans.apply_boxes(box_np, (mask.shape[-2], mask.shape[-1]))\n","      box_torch = torch.as_tensor(box, dtype=torch.float, device=self.device)\n","\n","\n","      sample = {\"embedding\": embedding,\n","                  'mask': mask,\n","                  'box' : box_torch\n","                  }\n","\n","      return sample\n","\n","\n","train = BraTSDataset(ground_truth_path, embedding_path, train_list)\n","val = BraTSDataset(ground_truth_path, embedding_path, val_list)\n","\n","\n","\n","train_loader = DataLoader(train, batch_size= 8, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val, batch_size= 4, shuffle=True, num_workers=0)\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","hprams = {\n","    \"num_epochs\": 15,\n","    \"best_loss\": 1e10,\n","    \"model_save_path\": \"/content\",\n","    \"lr\":1e-5,\n","    \"weight_decay\":0,\n","    \"device\": device,\n","    \"train_dataloader\": train_loader,\n","    \"val_dataloader\": val_loader\n","    }\n","\n","def eval(val_dataloader, sam_model, seg_loss):\n","  loss = 0\n","  for step_val, data in enumerate(val_dataloader):\n","    embedding, mask, box = data[\"embedding\"], data[\"mask\"], data[\"box\"]\n","\n","    with torch.no_grad():\n","      # embedding = sam_model.image_encoder(image)\n","      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","          points=None,\n","          boxes=box,\n","          masks=None,\n","      )\n","\n","      # predicted masks\n","      mask_predictions, _ = sam_model.mask_decoder(\n","      image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","      image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","      sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","      dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","      multimask_output=False,\n","      )\n","\n","      loss += seg_loss(mask_predictions, mask.to(device))\n","  loss /= step_val+1\n","  return loss\n","\n","\n","\n","def train(**kwargs):\n","    model_save_path = kwargs['model_save_path']\n","    num_epochs = kwargs['num_epochs']\n","    best_loss = kwargs['best_loss']\n","    lr = kwargs['lr']\n","    weight_decay = kwargs['weight_decay']\n","    device = kwargs['device']\n","    train_dataloader = kwargs['train_dataloader']\n","    # embedding_path = kwargs['embedding_path']\n","    val_dataloader = kwargs['val_dataloader']\n","\n","    os.makedirs(model_save_path, exist_ok=True)\n","\n","    sam_model.train()\n","    optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, factor=0.25,  patience=5)\n","    seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n","\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        # train\n","        for step, data in enumerate(tqdm(train_dataloader)):\n","            mask, box, embedding =  data[\"mask\"], data[\"box\"], data[\"embedding\"]\n","            embedding.to(device)\n","\n","            with torch.no_grad():\n","\n","              sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","                  points=None,\n","                  boxes=box,\n","                  masks=None,\n","              )\n","\n","            # predicted masks\n","            mask_predictions, _ = sam_model.mask_decoder(\n","              image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","              image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","              sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","              dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","              multimask_output=False,\n","            )\n","\n","            loss = seg_loss(mask_predictions, mask.to(device))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","            # break\n","        epoch_loss /= step+1\n","        scheduler.step(epoch_loss)\n","        val_loss = None\n","        if val_dataloader is not None: val_loss = eval(val_dataloader, sam_model, seg_loss)\n","        print(f'EPOCH: {epoch}, Loss: {epoch_loss}, Val loss {val_loss}')\n","        # wandb.log({\"epoch\": epoch, \"loss\": loss})\n","\n","        # save the latest model checkpoint\n","        # torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_'+str(epoch)+'.pth'))\n","\n","        # save the best model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_best.pth'))\n","\n","\n","\n","\n","train(**hprams)"],"metadata":{"id":"PgRPXFL04iVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2d input 3d mask\n","\n","print(\"No bounding box, flair_y\")\n","import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchvision.transforms import Compose, ToTensor, Resize\n","from torch.utils.data import Subset\n","\n","from tqdm import tqdm\n","from torchsummary import summary\n","import monai\n","from segment_anything import SamPredictor, sam_model_registry\n","from segment_anything.utils.transforms import ResizeLongestSide\n","# import wandb\n","import nibabel as nib\n","from segment_anything.utils.transforms import ResizeLongestSide\n","import torch.nn.functional as F\n","from skimage import transform, io, segmentation\n","\n","\n","torch.manual_seed(2023)\n","np.random.seed(2023)\n","\n","# change here\n","data_path = \"/content/out\"\n","\n","\n","def squarify(M,val = 0):\n","    (a,b)=M.shape\n","    if a>b:\n","        padding=((0,0),(0,a-b))\n","    else:\n","        padding=((0,b-a),(0,0))\n","    return np.pad(M,padding,mode='constant',constant_values=val)\n","\n","\n","\n","def get_bounding_box(mask):\n","  x2 = mask.shape[0]\n","  y2 = mask.shape[1]\n","\n","  return [0, 0, x2, y2]\n","\n","\n","def dice_loss(y_true, y_pred):\n","    \"\"\"\n","    implementation for generalized dice score loss\n","    GDL = 1 - (2 * sum(intersection)/(sum(pred)+sum(true)))\n","    \"\"\"\n","\n","    # y_pred = tf.round(y_pred)\n","    intersection = torch.sum(y_true * y_pred)\n","    pred_sum = torch.sum( y_pred)\n","    true_sum = torch.sum( y_true)\n","    smooth = torch.ones_like(intersection)*1e-5\n","    return 1-((2*intersection+smooth)/(pred_sum+true_sum+smooth))\n","\n","\n","model_type = 'vit_h'\n","checkpoint = 'sam_vit_h_4b8939.pth'\n","sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to('cuda')\n","\n","\n","\n","embedding_path = \"/content/BraTS_data/flair/z/embeddings\"\n","ground_truth_path = \"/content/out\"\n","\n","train_list = os.listdir(embedding_path)\n","train_list = train_list[:int(len(train_list)*0.7)]\n","val_list = train_list[int(len(train_list)*0.7):]\n","\n","class BraTSDataset(Dataset):\n","\n","    def __init__(self, ground_truth_path, embedding_path, list_of_data, device='cuda'):\n","        \"\"\"\n","        Args:\n","            embedding_path (string): Path to  images.\n","            masks_path (string): path to masks.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.ground_truth_path = ground_truth_path\n","        self.device = device\n","        self.list_of_data = list_of_data\n","        self.embedding_path = embedding_path\n","\n","    def __len__(self):\n","        return len(self.list_of_data)\n","\n","    def __getitem__(self, idx):\n","\n","\n","      embedding = np.load(self.embedding_path + \"/\" + self.list_of_data[idx])\n","      embedding = torch.as_tensor(embedding, dtype=torch.float, device=self.device)\n","\n","\n","      #--------------------------------------------------\n","      case_id = self.list_of_data[idx].split(\"_\")[1]\n","      slice_ = int(self.list_of_data[idx].split(\"_\")[-1].split(\".\")[0])\n","      # \"BraTS2021_00000\"\n","      case_name = \"BraTS2021_\"+ case_id\n","      img =  nib.load(self.ground_truth_path + \"/\" + case_name + \"/\"+ case_name +\"_seg.nii.gz\")\n","      img = img.get_fdata()\n","      mask = img[:,slice_,:]\n","\n","      mask[mask>0] = 1\n","      box = get_bounding_box(mask)\n","      # mask = squarify(mask)\n","\n","\n","\n","      mask = transform.resize(\n","                      mask,\n","                      (256, 256),\n","                      order=0,\n","                      preserve_range=True,\n","                      mode=\"constant\",\n","                  )\n","\n","\n","        # class_1 = np.zeros_like(mask)\n","        # class_2 = np.zeros_like(mask)\n","        # class_3 = np.zeros_like(mask)\n","\n","        # class_1[np.where(mask == 1)] = 1\n","        # class_2[np.where(mask == 2)] = 1\n","        # class_3[np.where(mask == 4)] = 1\n","\n","        # mask = mask = np.stack((class_1, class_2, class_3), axis = -1 )\n","      mask = torch.as_tensor(mask).to(self.device)\n","      mask = mask[None,...]\n","\n","\n","      box = get_bounding_box(mask)\n","      # box = [0,0,mask.shape[-2],mask.shape[-1]]\n","      box_np = np.array(box)\n","      sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)\n","      box = sam_trans.apply_boxes(box_np, (mask.shape[-2], mask.shape[-1]))\n","      box_torch = torch.as_tensor(box, dtype=torch.float, device=self.device)\n","\n","\n","      sample = {\"embedding\": embedding,\n","                  'mask': mask,\n","                  'box' : box_torch\n","                  }\n","\n","      return sample\n","\n","\n","train = BraTSDataset(ground_truth_path, embedding_path, train_list)\n","val = BraTSDataset(ground_truth_path, embedding_path, val_list)\n","\n","\n","\n","train_loader = DataLoader(train, batch_size= 8, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val, batch_size= 4, shuffle=True, num_workers=0)\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","hprams = {\n","    \"num_epochs\": 15,\n","    \"best_loss\": 1e10,\n","    \"model_save_path\": \"/content\",\n","    \"lr\":1e-5,\n","    \"weight_decay\":0,\n","    \"device\": device,\n","    \"train_dataloader\": train_loader,\n","    \"val_dataloader\": val_loader\n","    }\n","\n","def eval(val_dataloader, sam_model, seg_loss):\n","  loss = 0\n","  for step_val, data in enumerate(val_dataloader):\n","    embedding, mask, box = data[\"embedding\"], data[\"mask\"], data[\"box\"]\n","\n","    with torch.no_grad():\n","      # embedding = sam_model.image_encoder(image)\n","      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","          points=None,\n","          boxes=box,\n","          masks=None,\n","      )\n","\n","      # predicted masks\n","      mask_predictions, _ = sam_model.mask_decoder(\n","      image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","      image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","      sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","      dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","      multimask_output=False,\n","      )\n","\n","      loss += seg_loss(mask_predictions, mask.to(device))\n","  loss /= step_val+1\n","  return loss\n","\n","\n","\n","def train(**kwargs):\n","    model_save_path = kwargs['model_save_path']\n","    num_epochs = kwargs['num_epochs']\n","    best_loss = kwargs['best_loss']\n","    lr = kwargs['lr']\n","    weight_decay = kwargs['weight_decay']\n","    device = kwargs['device']\n","    train_dataloader = kwargs['train_dataloader']\n","    # embedding_path = kwargs['embedding_path']\n","    val_dataloader = kwargs['val_dataloader']\n","\n","    os.makedirs(model_save_path, exist_ok=True)\n","\n","    sam_model.train()\n","    optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, factor=0.25,  patience=5)\n","    seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n","\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        # train\n","        for step, data in enumerate(tqdm(train_dataloader)):\n","            mask, box, embedding =  data[\"mask\"], data[\"box\"], data[\"embedding\"]\n","            embedding.to(device)\n","\n","            with torch.no_grad():\n","\n","              sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","                  points=None,\n","                  boxes=box,\n","                  masks=None,\n","              )\n","\n","            # predicted masks\n","            mask_predictions, _ = sam_model.mask_decoder(\n","              image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","              image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","              sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","              dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","              multimask_output=False,\n","            )\n","\n","            loss = seg_loss(mask_predictions, mask.to(device))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","            # break\n","        epoch_loss /= step+1\n","        scheduler.step(epoch_loss)\n","        val_loss = None\n","        if val_dataloader is not None: val_loss = eval(val_dataloader, sam_model, seg_loss)\n","        print(f'EPOCH: {epoch}, Loss: {epoch_loss}, Val loss {val_loss}')\n","        # wandb.log({\"epoch\": epoch, \"loss\": loss})\n","\n","        # save the latest model checkpoint\n","        # torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_'+str(epoch)+'.pth'))\n","\n","        # save the best model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_best.pth'))\n","\n","\n","\n","\n","train(**hprams)"],"metadata":{"id":"TdNAtwaO4dM-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ldTVXBkl4dQh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pipeline for 3D embeddings"],"metadata":{"id":"wNz1lJFoaU11"}},{"cell_type":"code","source":["# 3d embedding saved\n","\n","print(\"\")\n","import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchvision.transforms import Compose, ToTensor, Resize\n","from torch.utils.data import Subset\n","\n","from tqdm import tqdm\n","from torchsummary import summary\n","import monai\n","from segment_anything import SamPredictor, sam_model_registry\n","from segment_anything.utils.transforms import ResizeLongestSide\n","# import wandb\n","import nibabel as nib\n","from segment_anything.utils.transforms import ResizeLongestSide\n","import torch.nn.functional as F\n","from skimage import transform, io, segmentation\n","import random\n","\n","\n","\n","torch.manual_seed(2023)\n","np.random.seed(2023)\n","\n","# change here\n","data_path = \"/content/out\"\n","\n","\n","def squarify(M,val = 0):\n","    (a,b)=M.shape\n","    if a>b:\n","        padding=((0,0),(0,a-b))\n","    else:\n","        padding=((0,b-a),(0,0))\n","    return np.pad(M,padding,mode='constant',constant_values=val)\n","\n","\n","\n","def get_bounding_box(mask):\n","  x2 = mask.shape[-2]\n","  y2 = mask.shape[-1]\n","\n","  return [0, 0, x2, y2]\n","\n","\n","def dice_loss(y_true, y_pred):\n","    \"\"\"\n","    implementation for generalized dice score loss\n","    GDL = 1 - (2 * sum(intersection)/(sum(pred)+sum(true)))\n","    \"\"\"\n","\n","    # y_pred = tf.round(y_pred)\n","    intersection = torch.sum(y_true * y_pred)\n","    pred_sum = torch.sum( y_pred)\n","    true_sum = torch.sum( y_true)\n","    smooth = torch.ones_like(intersection)*1e-5\n","    return 1-((2*intersection+smooth)/(pred_sum+true_sum+smooth))\n","\n","\n","model_type = 'vit_h'\n","checkpoint = 'sam_vit_h_4b8939.pth'\n","sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to('cuda')\n","\n","\n","\n","embedding_path = \"/content/BraTS_data/flair\"\n","ground_truth_path = \"/content/out\"\n","\n","train_list = os.listdir(embedding_path)\n","train_list = train_list[:int(len(train_list)*0.7)]\n","val_list = train_list[int(len(train_list)*0.7):]\n","\n","class BraTSDataset(Dataset):\n","\n","    def __init__(self, ground_truth_path, embedding_path, list_of_data, dimention, device='cuda'):\n","        \"\"\"\n","        Args:\n","            embedding_path (string): Path to  images.\n","            masks_path (string): path to masks.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.ground_truth_path = ground_truth_path\n","        self.device = device\n","        self.list_of_data = list_of_data\n","        self.embedding_path = embedding_path\n","        self.dimention = dimention\n","\n","    def __len__(self):\n","      return len(self.list_of_data)*240\n","\n","    def __getitem__(self, idx):\n","      case_num = idx % len(os.listdir(self.ground_truth_path))\n","      dimention = random.choice([\"x\", \"y\", \"z\"])\n","      case_name = sorted(os.listdir(self.ground_truth_path))[case_num]\n","\n","      img =  nib.load(self.ground_truth_path + \"/\" + case_name + \"/\"+ case_name +\"_seg.nii.gz\")\n","      img = img.get_fdata()\n","\n","      if dimention == \"z\":\n","        num_slices = img.shape[-1]\n","        slice_num = random.randint(0, num_slices)\n","        mask = img[:,:,slice_num]\n","      if dimention == \"y\":\n","        num_slices = img.shape[1]\n","        slice_num = random.randint(0, num_slices)\n","        mask = img[:,slice_num,:]\n","      if dimention == \"x\":\n","        num_slices = img.shape[0]\n","        slice_num = random.randint(0, num_slices)\n","        mask = img[slice_num,:,:]\n","\n","\n","      3d_embedding = np.load(self.embedding_path + \"/\" + dimention + \"/embeddings/\" + case_name + \".npy\")\n","      embedding = 3d_embedding[slice_num,:,:,:]\n","      embedding = torch.as_tensor(embedding, dtype=torch.float, device=self.device)\n","\n","      #--------------------------------------------------\n","\n","\n","      mask[mask>0] = 1\n","      if self.dimention != \"z\":\n","        mask = squarify(mask)\n","\n","      mask = transform.resize(\n","                      mask,\n","                      (256, 256),\n","                      order=0,\n","                      preserve_range=True,\n","                      mode=\"constant\",\n","                  )\n","\n","\n","        # class_1 = np.zeros_like(mask)\n","        # class_2 = np.zeros_like(mask)\n","        # class_3 = np.zeros_like(mask)\n","\n","        # class_1[np.where(mask == 1)] = 1\n","        # class_2[np.where(mask == 2)] = 1\n","        # class_3[np.where(mask == 4)] = 1\n","\n","        # mask = mask = np.stack((class_1, class_2, class_3), axis = -1 )\n","      mask = torch.as_tensor(mask).to(self.device)\n","      mask = mask[None,...]\n","\n","\n","      box = get_bounding_box(mask)\n","      # box = [0,0,mask.shape[-2],mask.shape[-1]]\n","      box_np = np.array(box)\n","      sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)\n","      box = sam_trans.apply_boxes(box_np, (mask.shape[-2], mask.shape[-1]))\n","      box_torch = torch.as_tensor(box, dtype=torch.float, device=self.device)\n","\n","\n","      sample = {\"embedding\": embedding,\n","                  'mask': mask,\n","                  'box' : box_torch\n","                  }\n","\n","      return sample\n","\n","\n","train = BraTSDataset(ground_truth_path, embedding_path, train_list)\n","val = BraTSDataset(ground_truth_path, embedding_path, val_list)\n","\n","\n","\n","train_loader = DataLoader(train, batch_size= 8, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val, batch_size= 4, shuffle=True, num_workers=0)\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","hprams = {\n","    \"num_epochs\": 15,\n","    \"best_loss\": 1e10,\n","    \"model_save_path\": \"/content\",\n","    \"lr\":1e-5,\n","    \"weight_decay\":0,\n","    \"device\": device,\n","    \"train_dataloader\": train_loader,\n","    \"val_dataloader\": val_loader\n","    }\n","\n","def eval(val_dataloader, sam_model, seg_loss):\n","  loss = 0\n","  for step_val, data in enumerate(val_dataloader):\n","    embedding, mask, box = data[\"embedding\"], data[\"mask\"], data[\"box\"]\n","\n","    with torch.no_grad():\n","      # embedding = sam_model.image_encoder(image)\n","      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","          points=None,\n","          boxes=box,\n","          masks=None,\n","      )\n","\n","      # predicted masks\n","      mask_predictions, _ = sam_model.mask_decoder(\n","      image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","      image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","      sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","      dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","      multimask_output=False,\n","      )\n","\n","      loss += seg_loss(mask_predictions, mask.to(device))\n","  loss /= step_val+1\n","  return loss\n","\n","\n","\n","def train(**kwargs):\n","    model_save_path = kwargs['model_save_path']\n","    num_epochs = kwargs['num_epochs']\n","    best_loss = kwargs['best_loss']\n","    lr = kwargs['lr']\n","    weight_decay = kwargs['weight_decay']\n","    device = kwargs['device']\n","    train_dataloader = kwargs['train_dataloader']\n","    # embedding_path = kwargs['embedding_path']\n","    val_dataloader = kwargs['val_dataloader']\n","\n","    os.makedirs(model_save_path, exist_ok=True)\n","\n","    sam_model.train()\n","    optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, factor=0.25,  patience=5)\n","    seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n","\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        # train\n","        for step, data in enumerate(tqdm(train_dataloader)):\n","            mask, box, embedding =  data[\"mask\"], data[\"box\"], data[\"embedding\"]\n","            embedding.to(device)\n","\n","            with torch.no_grad():\n","\n","              sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","                  points=None,\n","                  boxes=box,\n","                  masks=None,\n","              )\n","\n","            # predicted masks\n","            mask_predictions, _ = sam_model.mask_decoder(\n","              image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","              image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","              sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","              dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","              multimask_output=False,\n","            )\n","\n","            loss = seg_loss(mask_predictions, mask.to(device))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","            # break\n","        epoch_loss /= step+1\n","        scheduler.step(epoch_loss)\n","        val_loss = None\n","        if val_dataloader is not None: val_loss = eval(val_dataloader, sam_model, seg_loss)\n","        print(f'EPOCH: {epoch}, Loss: {epoch_loss}, Val loss {val_loss}')\n","        # wandb.log({\"epoch\": epoch, \"loss\": loss})\n","\n","        # save the latest model checkpoint\n","        # torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_'+str(epoch)+'.pth'))\n","\n","        # save the best model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_best.pth'))\n","\n","\n","\n","\n","train(**hprams)"],"metadata":{"id":"UocUGKdoaaUV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pipeline for saved 2d input and 3d mask bounding box"],"metadata":{"id":"L7LNejIaCHnC"}},{"cell_type":"code","source":["# 2d input 3d mask\n","\n","\n","\n","print(\"Bounding box, flair_z\")\n","import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchvision.transforms import Compose, ToTensor, Resize\n","from torch.utils.data import Subset\n","\n","from tqdm import tqdm\n","from torchsummary import summary\n","import monai\n","from segment_anything import SamPredictor, sam_model_registry\n","from segment_anything.utils.transforms import ResizeLongestSide\n","# import wandb\n","import nibabel as nib\n","from segment_anything.utils.transforms import ResizeLongestSide\n","import torch.nn.functional as F\n","from skimage import transform, io, segmentation\n","import random\n","\n","\n","torch.manual_seed(2023)\n","np.random.seed(2023)\n","\n","# change here\n","data_path = \"/content/out\"\n","\n","\n","def squarify(M,val = 0):\n","    (a,b)=M.shape\n","    if a>b:\n","        padding=((0,0),(0,a-b))\n","    else:\n","        padding=((0,b-a),(0,0))\n","    return np.pad(M,padding,mode='constant',constant_values=val)\n","\n","\n","\n","def get_bounding_box(mask):\n","\n","  y_indices, x_indices = np.where(mask > 0)\n","  if len(y_indices) == 0:\n","    return [0,0,0,0]\n","  x_min, x_max = np.min(x_indices), np.max(x_indices)\n","  y_min, y_max = np.min(y_indices), np.max(y_indices)\n","  # add perturbation to bounding box coordinates\n","  H, W = mask.shape\n","  x_min = max(0, x_min - np.random.randint(0, 20))\n","  x_max = min(W, x_max + np.random.randint(0, 20))\n","  y_min = max(0, y_min - np.random.randint(0, 20))\n","  y_max = min(H, y_max + np.random.randint(0, 20))\n","  # bboxes = np.array([x_min, y_min, x_max, y_max])\n","\n","  return [x_min, y_min, x_max, y_max]\n","\n","\n","def get_points(mask):\n","  mask= mask.cpu().numpy()\n","  mask = np.squeeze(mask, axis = 0)\n","  x = np.where(mask>0)\n","  if len(x[0]) == 0:\n","    return (0,0)\n","  all_points = list(zip(x[0], x[1]))\n","  return random.choices(all_points)\n","\n","\n","def dice_loss(y_true, y_pred):\n","    \"\"\"\n","    implementation for generalized dice score loss\n","    GDL = 1 - (2 * sum(intersection)/(sum(pred)+sum(true)))\n","    \"\"\"\n","\n","    # y_pred = tf.round(y_pred)\n","    intersection = torch.sum(y_true * y_pred)\n","    pred_sum = torch.sum( y_pred)\n","    true_sum = torch.sum( y_true)\n","    smooth = torch.ones_like(intersection)*1e-5\n","    return 1-((2*intersection+smooth)/(pred_sum+true_sum+smooth))\n","\n","\n","model_type = 'vit_h'\n","checkpoint = 'sam_vit_h_4b8939.pth'\n","sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to('cuda')\n","\n","\n","\n","embedding_path = \"/content/BraTS_data/flair/z/embeddings\"\n","ground_truth_path = \"/content/out\"\n","\n","train_list = os.listdir(embedding_path)\n","train_list = train_list[:int(len(train_list)*0.7)]\n","val_list = train_list[int(len(train_list)*0.7):]\n","\n","class BraTSDataset(Dataset):\n","\n","    def __init__(self, ground_truth_path, embedding_path, list_of_data, device='cuda'):\n","        \"\"\"\n","        Args:\n","            embedding_path (string): Path to  images.\n","            masks_path (string): path to masks.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.ground_truth_path = ground_truth_path\n","        self.device = device\n","        self.list_of_data = list_of_data\n","        self.embedding_path = embedding_path\n","\n","    def __len__(self):\n","        return len(self.list_of_data)\n","\n","    def __getitem__(self, idx):\n","\n","\n","      embedding = np.load(self.embedding_path + \"/\" + self.list_of_data[idx])\n","      embedding = torch.as_tensor(embedding, dtype=torch.float, device=self.device)\n","\n","\n","      #--------------------------------------------------\n","      case_id = self.list_of_data[idx].split(\"_\")[1]\n","      slice_ = int(self.list_of_data[idx].split(\"_\")[-1].split(\".\")[0])\n","      # \"BraTS2021_00000\"\n","      case_name = \"BraTS2021_\"+ case_id\n","      img =  nib.load(self.ground_truth_path + \"/\" + case_name + \"/\"+ case_name +\"_seg.nii.gz\")\n","      img = img.get_fdata()\n","      mask = img[:,:,slice_]\n","\n","      mask[mask>0] = 1\n","      # mask = squarify(mask)\n","\n","\n","\n","      mask = transform.resize(\n","                      mask,\n","                      (256, 256),\n","                      order=0,\n","                      preserve_range=True,\n","                      mode=\"constant\",\n","                  )\n","\n","        # class_1 = np.zeros_like(mask)\n","        # class_2 = np.zeros_like(mask)\n","        # class_3 = np.zeros_like(mask)\n","\n","        # class_1[np.where(mask == 1)] = 1\n","        # class_2[np.where(mask == 2)] = 1\n","        # class_3[np.where(mask == 4)] = 1\n","\n","        # mask = mask = np.stack((class_1, class_2, class_3), axis = -1 )\n","\n","\n","      box = get_bounding_box(mask)\n","      # box = [0,0,mask.shape[-2],mask.shape[-1]]\n","      box_np = np.array(box)\n","      sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)\n","      box = sam_trans.apply_boxes(box_np, (mask.shape[-2], mask.shape[-1]))\n","      box_torch = torch.as_tensor(box, dtype=torch.float, device=self.device)\n","\n","      mask = torch.as_tensor(mask).to(self.device)\n","      mask = mask[None,...]\n","\n","\n","      sample = {\"embedding\": embedding,\n","                  'mask': mask,\n","                  'box' : box_torch\n","                  }\n","\n","      return sample\n","\n","\n","train = BraTSDataset(ground_truth_path, embedding_path, train_list)\n","val = BraTSDataset(ground_truth_path, embedding_path, val_list)\n","\n","\n","\n","train_loader = DataLoader(train, batch_size= 2, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val, batch_size= 1, shuffle=True, num_workers=0)\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","hprams = {\n","    \"num_epochs\": 10,\n","    \"best_loss\": 1e10,\n","    \"model_save_path\": \"/content\",\n","    \"lr\":1e-4,\n","    \"weight_decay\":0,\n","    \"device\": device,\n","    \"train_dataloader\": train_loader,\n","    \"val_dataloader\": val_loader\n","    }\n","\n","def eval(val_dataloader, sam_model, seg_loss):\n","  loss = 0\n","  for step_val, data in enumerate(val_dataloader):\n","    embedding, mask, box = data[\"embedding\"], data[\"mask\"], data[\"box\"]\n","\n","    with torch.no_grad():\n","      # embedding = sam_model.image_encoder(image)\n","      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","          points=None,\n","          boxes=box,\n","          masks=None,\n","      )\n","\n","      # predicted masks\n","      mask_predictions, _ = sam_model.mask_decoder(\n","      image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","      image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","      sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","      dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","      multimask_output=False,\n","      )\n","\n","      loss += seg_loss(mask_predictions, mask.to(device))\n","  loss /= step_val+1\n","  return loss\n","\n","\n","\n","def train(**kwargs):\n","    model_save_path = kwargs['model_save_path']\n","    num_epochs = kwargs['num_epochs']\n","    best_loss = kwargs['best_loss']\n","    lr = kwargs['lr']\n","    weight_decay = kwargs['weight_decay']\n","    device = kwargs['device']\n","    train_dataloader = kwargs['train_dataloader']\n","    # embedding_path = kwargs['embedding_path']\n","    val_dataloader = kwargs['val_dataloader']\n","\n","    os.makedirs(model_save_path, exist_ok=True)\n","\n","    sam_model.train()\n","    optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, factor=0.25,  patience=3)\n","    seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n","\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        # train\n","        for step, data in enumerate(tqdm(train_dataloader)):\n","            mask, box, embedding =  data[\"mask\"], data[\"box\"], data[\"embedding\"]\n","\n","            embedding.to(device)\n","\n","            with torch.no_grad():\n","\n","              sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","                  points=None,\n","                  boxes=box,\n","                  masks=None,\n","              )\n","\n","            # predicted masks\n","            mask_predictions, _ = sam_model.mask_decoder(\n","              image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","              image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","              sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","              dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","              multimask_output=False,\n","            )\n","\n","            loss = seg_loss(mask_predictions, mask.to(device))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","            # break\n","        epoch_loss /= step+1\n","        scheduler.step(epoch_loss)\n","        val_loss = None\n","        if val_dataloader is not None: val_loss = eval(val_dataloader, sam_model, seg_loss)\n","        print(f'EPOCH: {epoch}, Loss: {epoch_loss}, Val loss {val_loss}')\n","        # wandb.log({\"epoch\": epoch, \"loss\": loss})\n","\n","        # save the latest model checkpoint\n","        # torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_'+str(epoch)+'.pth'))\n","\n","        # save the best model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_best.pth'))\n","\n","\n","\n","\n","train(**hprams)"],"metadata":{"id":"b68SbjqhNU0J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3D embedding for all modalities"],"metadata":{"id":"yWr9Wbfy1QbJ"}},{"cell_type":"code","source":["# 3d embedding saved  #binary\n","\n","print(\"\")\n","import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchvision.transforms import Compose, ToTensor, Resize\n","from torch.utils.data import Subset\n","\n","from tqdm import tqdm\n","from torchsummary import summary\n","import monai\n","from segment_anything import SamPredictor, sam_model_registry\n","from segment_anything.utils.transforms import ResizeLongestSide\n","# import wandb\n","import nibabel as nib\n","from segment_anything.utils.transforms import ResizeLongestSide\n","import torch.nn.functional as F\n","from skimage import transform, io, segmentation\n","import random\n","\n","\n","\n","torch.manual_seed(2023)\n","np.random.seed(2023)\n","\n","# change here\n","data_path = \"/content/out\"\n","\n","\n","def squarify(M,val = 0):\n","    (a,b)=M.shape\n","    if a>b:\n","        padding=((0,0),(0,a-b))\n","    else:\n","        padding=((0,b-a),(0,0))\n","    return np.pad(M,padding,mode='constant',constant_values=val)\n","\n","\n","\n","def get_bounding_box(mask):\n","\n","  y_indices, x_indices = np.where(mask > 0)\n","  if len(y_indices) == 0:\n","    return [0,0,0,0]\n","  x_min, x_max = np.min(x_indices), np.max(x_indices)\n","  y_min, y_max = np.min(y_indices), np.max(y_indices)\n","  # add perturbation to bounding box coordinates\n","  H, W = mask.shape\n","  x_min = max(0, x_min - np.random.randint(0, 20))\n","  x_max = min(W, x_max + np.random.randint(0, 20))\n","  y_min = max(0, y_min - np.random.randint(0, 20))\n","  y_max = min(H, y_max + np.random.randint(0, 20))\n","  # bboxes = np.array([x_min, y_min, x_max, y_max])\n","\n","  return [x_min, y_min, x_max, y_max]\n","\n","\n","def get_cases_list(input_base, masks_path):\n","  images = []\n","  for i in os.listdir(masks_path):\n","  # for i in  [\"BraTS-SSA-00002-000\"]:\n","    img =  nib.load(masks_path + \"/\" + i + \"/\"+ i +\"-seg.nii.gz\")\n","    img = img.get_fdata()\n","    sclices_z = img.shape[2]\n","    sclices_y = img.shape[1]\n","    sclices_x = img.shape[0]\n","    for z in range(sclices_z):\n","      if img[:,:,z].max() != 0:\n","        images.append((i ,z, \"flair/z\"))\n","        images.append((i ,z, \"t1/z\"))\n","        images.append((i ,z, \"t2/z\"))\n","        images.append((i ,z, \"t1ce/z\"))\n","\n","    for y in range(sclices_y):\n","      if img[:,y,:].max() != 0:\n","        images.append((i ,y, \"flair/y\"))\n","        images.append((i ,y, \"t1/y\"))\n","        images.append((i ,y, \"t2/y\"))\n","        images.append((i ,y, \"t1ce/y\"))\n","\n","\n","    for x in range(sclices_x):\n","      if img[x,:,:].max() != 0:\n","        images.append(( i ,x, \"flair/x\"))\n","        images.append(( i ,x, \"t1/x\"))\n","        images.append((i ,x, \"t2/x\"))\n","        images.append(( i ,x, \"t1ce/x\"))\n","\n","    return images\n","\n","\n","def dice_loss(y_true, y_pred):\n","    \"\"\"\n","    implementation for generalized dice score loss\n","    GDL = 1 - (2 * sum(intersection)/(sum(pred)+sum(true)))\n","    \"\"\"\n","\n","    # y_pred = tf.round(y_pred)\n","    intersection = torch.sum(y_true * y_pred)\n","    pred_sum = torch.sum( y_pred)\n","    true_sum = torch.sum( y_true)\n","    smooth = torch.ones_like(intersection)*1e-5\n","    return 1-((2*intersection+smooth)/(pred_sum+true_sum+smooth))\n","\n","\n","model_type = 'vit_h'\n","checkpoint = 'sam_vit_h_4b8939.pth'\n","sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to('cuda')\n","\n","\n","\n","embedding_path = \"/content/BraTS_data\"\n","ground_truth_path = \"/content/drive/MyDrive/SSA\"\n","\n","data = get_cases_list(embedding_path, ground_truth_path)\n","print(len(data))\n","train_list = data[:int(len(data)*0.7)]\n","val_list = data[int(len(data)*0.7):]\n","\n","class BraTSDataset(Dataset):\n","\n","    def __init__(self, ground_truth_path, embedding_path, list_of_data, device='cuda'):\n","        \"\"\"\n","        Args:\n","            embedding_path (string): Path to  images.\n","            masks_path (string): path to masks.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.ground_truth_path = ground_truth_path\n","        self.device = device\n","        self.list_of_data = list_of_data\n","        self.embedding_path = embedding_path\n","\n","    def __len__(self):\n","      return len(self.list_of_data)\n","\n","    def __getitem__(self, idx):\n","      case_name, slic, d = self.list_of_data[idx]\n","\n","      img =  nib.load(self.ground_truth_path + \"/\" + case_name.split(\".\")[0] + \"/\"+ case_name.split(\".\")[0] +\"-seg.nii.gz\")\n","      img = img.get_fdata()\n","\n","      dim =  d.split(\"/\")[-1]\n","      if dim == \"x\":\n","        mask = img[slic,:,:]\n","      elif dim == \"y\":\n","        mask = img[:,slic,:]\n","      elif dim == \"z\":\n","        mask = img[:,:,slic]\n","\n","\n","      d_embedding = np.load(self.embedding_path + \"/\" + d + \"/embeddings/\" + case_name + \".npy\")\n","      embedding = d_embedding[:,:,:, slic]\n","      embedding = torch.as_tensor(embedding, dtype=torch.float, device=self.device)\n","\n","      #--------------------------------------------------\n","      mask[mask>0] = 1\n","      mask = transform.resize(\n","                      mask,\n","                      (256, 256),\n","                      order=0,\n","                      preserve_range=True,\n","                      mode=\"constant\",\n","                  )\n","\n","      box = get_bounding_box(mask)\n","      box_np = np.array(box)\n","      sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)\n","      box = sam_trans.apply_boxes(box_np, (mask.shape[-2], mask.shape[-1]))\n","      box_torch = torch.as_tensor(box, dtype=torch.float, device=self.device)\n","\n","      # class_1 = np.zeros_like(mask)\n","      # class_2 = np.zeros_like(mask)\n","      # class_3 = np.zeros_like(mask)\n","\n","      # class_1[np.where(mask == 1)] = 1\n","      # class_2[np.where(mask == 2)] = 1\n","      # class_3[np.where(mask == 3)] = 1\n","\n","      # mask = mask = np.stack((class_1, class_2, class_3), axis = -1 )\n","\n","\n","      mask = torch.as_tensor(mask).to(self.device)\n","      mask = mask[None,...]\n","\n","      sample = {\"embedding\": embedding,\n","                  'mask': mask,\n","                  'box' : box_torch\n","                  }\n","\n","      return sample\n","\n","\n","train = BraTSDataset(ground_truth_path, embedding_path, train_list)\n","val = BraTSDataset(ground_truth_path, embedding_path, val_list)\n","\n","\n","train_loader = DataLoader(train, batch_size= 8, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val, batch_size= 4, shuffle=True, num_workers=0)\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","hprams = {\n","    \"num_epochs\": 5,\n","    \"best_loss\": 1e10,\n","    \"model_save_path\": \"/content\",\n","    \"lr\":1e-5,\n","    \"weight_decay\":0,\n","    \"device\": device,\n","    \"train_dataloader\": train_loader,\n","    \"val_dataloader\": val_loader\n","    }\n","\n","def eval(val_dataloader, sam_model, seg_loss):\n","  loss = 0\n","  for step_val, data in enumerate(val_dataloader):\n","    embedding, mask, box = data[\"embedding\"], data[\"mask\"], data[\"box\"]\n","\n","    with torch.no_grad():\n","      # embedding = sam_model.image_encoder(image)\n","      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","          points=None,\n","          boxes=box,\n","          masks=None,\n","      )\n","\n","      # predicted masks\n","      mask_predictions, _ = sam_model.mask_decoder(\n","      image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","      image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","      sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","      dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","      multimask_output=False,\n","      )\n","\n","      loss += seg_loss(mask_predictions, mask.to(device))\n","  loss /= step_val+1\n","  return loss\n","\n","\n","\n","def train(**kwargs):\n","    model_save_path = kwargs['model_save_path']\n","    num_epochs = kwargs['num_epochs']\n","    best_loss = kwargs['best_loss']\n","    lr = kwargs['lr']\n","    weight_decay = kwargs['weight_decay']\n","    device = kwargs['device']\n","    train_dataloader = kwargs['train_dataloader']\n","    # embedding_path = kwargs['embedding_path']\n","    val_dataloader = kwargs['val_dataloader']\n","\n","    os.makedirs(model_save_path, exist_ok=True)\n","\n","    sam_model.train()\n","    optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, factor=0.25,  patience=5)\n","    seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n","\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        # train\n","        for step, data in enumerate(tqdm(train_dataloader)):\n","            mask, box, embedding =  data[\"mask\"], data[\"box\"], data[\"embedding\"]\n","            embedding.to(device)\n","\n","            with torch.no_grad():\n","\n","              sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","                  points=None,\n","                  boxes=box,\n","                  masks=None,\n","              )\n","\n","            # predicted masks\n","            mask_predictions, _ = sam_model.mask_decoder(\n","              image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","              image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","              sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","              dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","              multimask_output=False,\n","            )\n","\n","            loss = seg_loss(mask_predictions, mask.to(device))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","            # break\n","        epoch_loss /= step+1\n","        scheduler.step(epoch_loss)\n","        val_loss = None\n","        if val_dataloader is not None: val_loss = eval(val_dataloader, sam_model, seg_loss)\n","        print(f'EPOCH: {epoch}, Loss: {epoch_loss}, Val loss {val_loss}')\n","        # wandb.log({\"epoch\": epoch, \"loss\": loss})\n","\n","        # save the latest model checkpoint\n","        # torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_'+str(epoch)+'.pth'))\n","\n","        # save the best model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_best.pth'))\n","\n","\n","\n","\n","train(**hprams)"],"metadata":{"id":"L5Z6TCNi1Vn6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3d embedding saved  # multiclass\n","\n","print(\"\")\n","import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchvision.transforms import Compose, ToTensor, Resize\n","from torch.utils.data import Subset\n","\n","from tqdm import tqdm\n","from torchsummary import summary\n","import monai\n","from segment_anything import SamPredictor, sam_model_registry\n","from segment_anything.utils.transforms import ResizeLongestSide\n","# import wandb\n","import nibabel as nib\n","from segment_anything.utils.transforms import ResizeLongestSide\n","import torch.nn.functional as F\n","from skimage import transform, io, segmentation\n","import random\n","\n","\n","\n","torch.manual_seed(2023)\n","np.random.seed(2023)\n","\n","# change here\n","data_path = \"/content/out\"\n","\n","\n","def squarify(M,val = 0):\n","    (a,b)=M.shape\n","    if a>b:\n","        padding=((0,0),(0,a-b))\n","    else:\n","        padding=((0,b-a),(0,0))\n","    return np.pad(M,padding,mode='constant',constant_values=val)\n","\n","\n","\n","def get_bounding_box(mask):\n","\n","  y_indices, x_indices = np.where(mask > 0)\n","  if len(y_indices) == 0:\n","    return [0,0,0,0]\n","  x_min, x_max = np.min(x_indices), np.max(x_indices)\n","  y_min, y_max = np.min(y_indices), np.max(y_indices)\n","  # add perturbation to bounding box coordinates\n","  H, W = mask.shape\n","  x_min = max(0, x_min - np.random.randint(0, 20))\n","  x_max = min(W, x_max + np.random.randint(0, 20))\n","  y_min = max(0, y_min - np.random.randint(0, 20))\n","  y_max = min(H, y_max + np.random.randint(0, 20))\n","  # bboxes = np.array([x_min, y_min, x_max, y_max])\n","\n","  return [x_min, y_min, x_max, y_max]\n","\n","\n","def get_cases_list(input_base, masks_path):\n","  images = []\n","  for i in os.listdir(masks_path):\n","  # for i in  [\"BraTS-SSA-00002-000\"]:\n","    img =  nib.load(masks_path + \"/\" + i + \"/\"+ i +\"-seg.nii.gz\")\n","    img = img.get_fdata()\n","    sclices_z = img.shape[2]\n","    sclices_y = img.shape[1]\n","    sclices_x = img.shape[0]\n","    for z in range(sclices_z):\n","      if img[:,:,z].max() != 0:\n","        images.append((i ,z, \"flair/z\"))\n","        images.append((i ,z, \"t1/z\"))\n","        images.append((i ,z, \"t2/z\"))\n","        images.append((i ,z, \"t1ce/z\"))\n","\n","    for y in range(sclices_y):\n","      if img[:,y,:].max() != 0:\n","        images.append((i ,y, \"flair/y\"))\n","        images.append((i ,y, \"t1/y\"))\n","        images.append((i ,y, \"t2/y\"))\n","        images.append((i ,y, \"t1ce/y\"))\n","\n","\n","    for x in range(sclices_x):\n","      if img[x,:,:].max() != 0:\n","        images.append(( i ,x, \"flair/x\"))\n","        images.append(( i ,x, \"t1/x\"))\n","        images.append((i ,x, \"t2/x\"))\n","        images.append(( i ,x, \"t1ce/x\"))\n","\n","    return images\n","\n","\n","def dice_loss(y_true, y_pred):\n","    \"\"\"\n","    implementation for generalized dice score loss\n","    GDL = 1 - (2 * sum(intersection)/(sum(pred)+sum(true)))\n","    \"\"\"\n","\n","    # y_pred = tf.round(y_pred)\n","    intersection = torch.sum(y_true * y_pred)\n","    pred_sum = torch.sum( y_pred)\n","    true_sum = torch.sum( y_true)\n","    smooth = torch.ones_like(intersection)*1e-5\n","    return 1-((2*intersection+smooth)/(pred_sum+true_sum+smooth))\n","\n","\n","model_type = 'vit_h'\n","checkpoint = 'sam_vit_h_4b8939.pth'\n","sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to('cuda')\n","\n","\n","\n","embedding_path = \"/content/BraTS_data\"\n","ground_truth_path = \"/content/drive/MyDrive/SSA\"\n","\n","data = get_cases_list(embedding_path, ground_truth_path)\n","print(len(data))\n","train_list = data[:int(len(data)*0.7)]\n","val_list = data[int(len(data)*0.7):]\n","\n","class BraTSDataset(Dataset):\n","\n","    def __init__(self, ground_truth_path, embedding_path, list_of_data, device='cuda'):\n","        \"\"\"\n","        Args:\n","            embedding_path (string): Path to  images.\n","            masks_path (string): path to masks.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.ground_truth_path = ground_truth_path\n","        self.device = device\n","        self.list_of_data = list_of_data\n","        self.embedding_path = embedding_path\n","\n","    def __len__(self):\n","      return len(self.list_of_data)\n","\n","    def __getitem__(self, idx):\n","      case_name, slic, d = self.list_of_data[idx]\n","\n","      img =  nib.load(self.ground_truth_path + \"/\" + case_name.split(\".\")[0] + \"/\"+ case_name.split(\".\")[0] +\"-seg.nii.gz\")\n","      img = img.get_fdata()\n","\n","      dim =  d.split(\"/\")[-1]\n","      if dim == \"x\":\n","        mask = img[slic,:,:]\n","      elif dim == \"y\":\n","        mask = img[:,slic,:]\n","      elif dim == \"z\":\n","        mask = img[:,:,slic]\n","\n","\n","      d_embedding = np.load(self.embedding_path + \"/\" + d + \"/embeddings/\" + case_name + \".npy\")\n","      embedding = d_embedding[:,:,:, slic]\n","      embedding = torch.as_tensor(embedding, dtype=torch.float, device=self.device)\n","\n","      #--------------------------------------------------\n","      # mask[mask>0] = 1\n","      mask = transform.resize(\n","                      mask,\n","                      (256, 256),\n","                      order=0,\n","                      preserve_range=True,\n","                      mode=\"constant\",\n","                  )\n","\n","      box = get_bounding_box(mask)\n","      box_np = np.array(box)\n","      sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)\n","      box = sam_trans.apply_boxes(box_np, (mask.shape[-2], mask.shape[-1]))\n","      box_torch = torch.as_tensor(box, dtype=torch.float, device=self.device)\n","\n","      class_1 = np.zeros_like(mask)\n","      class_2 = np.zeros_like(mask)\n","      class_3 = np.zeros_like(mask)\n","\n","      class_1[np.where(mask == 1)] = 1\n","      class_2[np.where(mask == 2)] = 1\n","      class_3[np.where(mask == 3)] = 1\n","\n","      mask = mask = np.stack((class_1, class_2, class_3), axis = 0)\n","\n","\n","      mask = torch.as_tensor(mask).to(self.device)\n","      # mask = mask[None,...]\n","\n","      sample = {\"embedding\": embedding,\n","                  'mask': mask,\n","                  'box' : box_torch\n","                  }\n","\n","      return sample\n","\n","\n","train = BraTSDataset(ground_truth_path, embedding_path, train_list)\n","val = BraTSDataset(ground_truth_path, embedding_path, val_list)\n","\n","\n","train_loader = DataLoader(train, batch_size= 8, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val, batch_size= 4, shuffle=True, num_workers=0)\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","hprams = {\n","    \"num_epochs\": 5,\n","    \"best_loss\": 1e10,\n","    \"model_save_path\": \"/content\",\n","    \"lr\":1e-5,\n","    \"weight_decay\":0,\n","    \"device\": device,\n","    \"train_dataloader\": train_loader,\n","    \"val_dataloader\": val_loader\n","    }\n","\n","def eval(val_dataloader, sam_model, seg_loss):\n","  loss = 0\n","  for step_val, data in enumerate(val_dataloader):\n","    embedding, mask, box = data[\"embedding\"], data[\"mask\"], data[\"box\"]\n","\n","    with torch.no_grad():\n","      # embedding = sam_model.image_encoder(image)\n","      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","          points=None,\n","          boxes=box,\n","          masks=None,\n","      )\n","\n","      # predicted masks\n","      mask_predictions, _ = sam_model.mask_decoder(\n","      image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","      image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","      sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","      dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","      multimask_output=True,\n","      )\n","\n","      loss += seg_loss(mask_predictions, mask.to(device))\n","  loss /= step_val+1\n","  return loss\n","\n","\n","\n","def train(**kwargs):\n","    model_save_path = kwargs['model_save_path']\n","    num_epochs = kwargs['num_epochs']\n","    best_loss = kwargs['best_loss']\n","    lr = kwargs['lr']\n","    weight_decay = kwargs['weight_decay']\n","    device = kwargs['device']\n","    train_dataloader = kwargs['train_dataloader']\n","    # embedding_path = kwargs['embedding_path']\n","    val_dataloader = kwargs['val_dataloader']\n","\n","    os.makedirs(model_save_path, exist_ok=True)\n","\n","    sam_model.train()\n","    optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, factor=0.25,  patience=5)\n","    seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n","\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        # train\n","        for step, data in enumerate(tqdm(train_dataloader)):\n","            mask, box, embedding =  data[\"mask\"], data[\"box\"], data[\"embedding\"]\n","            embedding.to(device)\n","\n","            with torch.no_grad():\n","\n","              sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","                  points=None,\n","                  boxes=box,\n","                  masks=None,\n","              )\n","\n","            # predicted masks\n","            mask_predictions, _ = sam_model.mask_decoder(\n","              image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","              image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","              sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","              dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","              multimask_output=True,\n","            )\n","\n","            loss = seg_loss(mask_predictions, mask.to(device))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","            # break\n","        epoch_loss /= step+1\n","        scheduler.step(epoch_loss)\n","        val_loss = None\n","        if val_dataloader is not None: val_loss = eval(val_dataloader, sam_model, seg_loss)\n","        print(f'EPOCH: {epoch}, Loss: {epoch_loss}, Val loss {val_loss}')\n","        # wandb.log({\"epoch\": epoch, \"loss\": loss})\n","\n","        # save the latest model checkpoint\n","        # torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_'+str(epoch)+'.pth'))\n","\n","        # save the best model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_best.pth'))\n","\n","\n","\n","\n","train(**hprams)"],"metadata":{"id":"98QFjJOA2ow_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimized 3d for all modalities"],"metadata":{"id":"-2hV8fiBPHgb"}},{"cell_type":"code","source":["# 3d embedding saved  #multi class\n","\n","print(\"\")\n","import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchvision.transforms import Compose, ToTensor, Resize\n","from torch.utils.data import Subset\n","\n","from tqdm import tqdm\n","from torchsummary import summary\n","import monai\n","from segment_anything import SamPredictor, sam_model_registry\n","from segment_anything.utils.transforms import ResizeLongestSide\n","# import wandb\n","import nibabel as nib\n","from segment_anything.utils.transforms import ResizeLongestSide\n","import torch.nn.functional as F\n","from skimage import transform, io, segmentation\n","import random\n","from monai.data import ThreadDataLoader, CacheDataset\n","\n","\n","\n","torch.manual_seed(2023)\n","np.random.seed(2023)\n","\n","\n","\n","def squarify(M,val = 0):\n","    (a,b)=M.shape\n","    if a>b:\n","        padding=((0,0),(0,a-b))\n","    else:\n","        padding=((0,b-a),(0,0))\n","    return np.pad(M,padding,mode='constant',constant_values=val)\n","\n","\n","\n","def get_bounding_box(mask):\n","\n","  y_indices, x_indices = np.where(mask > 0)\n","  if len(y_indices) == 0:\n","    return [0,0,0,0]\n","  x_min, x_max = np.min(x_indices), np.max(x_indices)\n","  y_min, y_max = np.min(y_indices), np.max(y_indices)\n","  # add perturbation to bounding box coordinates\n","  H, W = mask.shape\n","  x_min = max(0, x_min - np.random.randint(0, 20))\n","  x_max = min(W, x_max + np.random.randint(0, 20))\n","  y_min = max(0, y_min - np.random.randint(0, 20))\n","  y_max = min(H, y_max + np.random.randint(0, 20))\n","  # bboxes = np.array([x_min, y_min, x_max, y_max])\n","\n","  return [x_min, y_min, x_max, y_max]\n","\n","\n","def get_cases_list(input_base, masks_path):\n","  images = []\n","  for i in os.listdir(masks_path):\n","  # for i in  [\"BraTS-SSA-00002-000\"]:\n","    img =  nib.load(masks_path + \"/\" + i + \"/\"+ i +\"-seg.nii.gz\")\n","    img = img.get_fdata()\n","    sclices_z = img.shape[2]\n","    sclices_y = img.shape[1]\n","    sclices_x = img.shape[0]\n","    for z in range(sclices_z):\n","      if img[:,:,z].max() != 0:\n","        images.append((i ,z, \"flair/z\"))\n","        images.append((i ,z, \"t1/z\"))\n","        images.append((i ,z, \"t2/z\"))\n","        images.append((i ,z, \"t1ce/z\"))\n","\n","    for y in range(sclices_y):\n","      if img[:,y,:].max() != 0:\n","        images.append((i ,y, \"flair/y\"))\n","        images.append((i ,y, \"t1/y\"))\n","        images.append((i ,y, \"t2/y\"))\n","        images.append((i ,y, \"t1ce/y\"))\n","\n","\n","    for x in range(sclices_x):\n","      if img[x,:,:].max() != 0:\n","        images.append(( i ,x, \"flair/x\"))\n","        images.append(( i ,x, \"t1/x\"))\n","        images.append((i ,x, \"t2/x\"))\n","        images.append(( i ,x, \"t1ce/x\"))\n","\n","    return images\n","\n","\n","def dice_loss(y_true, y_pred):\n","    \"\"\"\n","    implementation for generalized dice score loss\n","    GDL = 1 - (2 * sum(intersection)/(sum(pred)+sum(true)))\n","    \"\"\"\n","\n","    # y_pred = tf.round(y_pred)\n","    intersection = torch.sum(y_true * y_pred)\n","    pred_sum = torch.sum( y_pred)\n","    true_sum = torch.sum( y_true)\n","    smooth = torch.ones_like(intersection)*1e-5\n","    return 1-((2*intersection+smooth)/(pred_sum+true_sum+smooth))\n","\n","\n","model_type = 'vit_h'\n","checkpoint = 'sam_vit_h_4b8939.pth'\n","sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to('cuda')\n","\n","\n","\n","embedding_path = \"/content/BraTS_data\"\n","ground_truth_path = \"/content/drive/MyDrive/SSA\"\n","\n","data = get_cases_list(embedding_path, ground_truth_path)\n","print(len(data))\n","\n","train_list = data[:int(len(data)*0.8)]\n","val_list = data[int(len(data)*0.8):]\n","\n","class BraTSDataset(CacheDataset):\n","\n","    def __init__(self, ground_truth_path, embedding_path, list_of_data, device='cuda'):\n","        \"\"\"\n","        Args:\n","            embedding_path (string): Path to  images.\n","            masks_path (string): path to masks.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.ground_truth_path = ground_truth_path\n","        self.device = device\n","        self.list_of_data = list_of_data\n","        self.embedding_path = embedding_path\n","\n","    def __len__(self):\n","      return len(self.list_of_data)\n","\n","    def __getitem__(self, idx):\n","      case_name, slic, d = self.list_of_data[idx]\n","\n","      img =  nib.load(self.ground_truth_path + \"/\" + case_name.split(\".\")[0] + \"/\"+ case_name.split(\".\")[0] +\"-seg.nii.gz\")\n","      img = img.get_fdata()\n","\n","      dim =  d.split(\"/\")[-1]\n","      if dim == \"x\":\n","        mask = img[slic,:,:]\n","      elif dim == \"y\":\n","        mask = img[:,slic,:]\n","      elif dim == \"z\":\n","        mask = img[:,:,slic]\n","\n","\n","      d_embedding = np.load(self.embedding_path + \"/\" + d + \"/embeddings/\" + case_name + \".npy\")\n","      embedding = d_embedding[:,:,:, slic]\n","      embedding = torch.as_tensor(embedding, dtype=torch.float)#, device=self.device)\n","\n","      #--------------------------------------------------\n","      # mask[mask>0] = 1\n","      mask = transform.resize(\n","                      mask,\n","                      (256, 256),\n","                      order=0,\n","                      preserve_range=True,\n","                      mode=\"constant\",\n","                  )\n","\n","      box = get_bounding_box(mask)\n","      box_np = np.array(box)\n","      sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)\n","      box = sam_trans.apply_boxes(box_np, (mask.shape[-2], mask.shape[-1]))\n","      box_torch = torch.as_tensor(box, dtype=torch.float)#, device=self.device)\n","\n","      class_1 = np.zeros_like(mask)\n","      class_2 = np.zeros_like(mask)\n","      class_3 = np.zeros_like(mask)\n","\n","      class_1[np.where(mask == 1)] = 1\n","      class_2[np.where(mask == 2)] = 1\n","      class_3[np.where(mask == 3)] = 1\n","\n","      mask = mask = np.stack((class_1, class_2, class_3), axis = 0 )\n","\n","\n","      mask = torch.as_tensor(mask)#.to(self.device)\n","      # mask = mask[None,...]\n","\n","      sample = {\"embedding\": embedding,\n","                  'mask': mask,\n","                  'box' : box_torch\n","                  }\n","\n","      return sample\n","\n","\n","train = BraTSDataset(ground_truth_path, embedding_path, train_list)\n","val = BraTSDataset(ground_truth_path, embedding_path, val_list)\n","\n","\n","train_loader = ThreadDataLoader(train, batch_size= 2, shuffle=True, num_workers=2)\n","val_loader = ThreadDataLoader(val, batch_size= 2, shuffle=True, num_workers=2)\n","\n","# print(next(iter(train_loader)))\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","hprams = {\n","    \"num_epochs\": 5,\n","    \"best_loss\": 1e10,\n","    \"model_save_path\": \"/content\",\n","    \"lr\":1e-5,\n","    \"weight_decay\":0,\n","    \"device\": device,\n","    \"train_dataloader\": train_loader,\n","    \"val_dataloader\": val_loader\n","    }\n","\n","def eval(val_dataloader, sam_model, seg_loss):\n","  loss = 0\n","  for step_val, data in enumerate(val_dataloader):\n","    embedding, mask, box = data[\"embedding\"], data[\"mask\"], data[\"box\"]\n","\n","    with torch.no_grad():\n","      with torch.cuda.amp.autocast():\n","        # embedding = sam_model.image_encoder(image)\n","        sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","            points=None,\n","            boxes=box.to(device),\n","            masks=None,\n","        )\n","\n","        # predicted masks\n","        mask_predictions, _ = sam_model.mask_decoder(\n","        image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","        image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","        multimask_output=True,\n","        )\n","\n","        loss += seg_loss(mask_predictions, mask.to(device))\n","  loss /= step_val+1\n","  return loss\n","\n","\n","\n","def train(**kwargs):\n","    model_save_path = kwargs['model_save_path']\n","    num_epochs = kwargs['num_epochs']\n","    best_loss = kwargs['best_loss']\n","    lr = kwargs['lr']\n","    weight_decay = kwargs['weight_decay']\n","    device = kwargs['device']\n","    train_dataloader = kwargs['train_dataloader']\n","    # embedding_path = kwargs['embedding_path']\n","    val_dataloader = kwargs['val_dataloader']\n","\n","    os.makedirs(model_save_path, exist_ok=True)\n","\n","    sam_model.train()\n","    optimizer = monai.optimizers.Novograd(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, factor=0.25,  patience=5)\n","    seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n","\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        # train\n","        for step, data in enumerate(tqdm(train_dataloader)):\n","            mask, box, embedding =  data[\"mask\"], data[\"box\"], data[\"embedding\"]\n","            # embedding.to(device)\n","\n","            with torch.no_grad():\n","              with torch.cuda.amp.autocast():\n","\n","                sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","                    points=None,\n","                    boxes=box.to(device),\n","                    masks=None,\n","                )\n","            with torch.cuda.amp.autocast():\n","            # predicted masks\n","              mask_predictions, _ = sam_model.mask_decoder(\n","                image_embeddings= embedding.to(device), # (B, 256, 64, 64)\n","                image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n","                sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n","                dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n","                multimask_output=True,\n","              )\n","\n","              loss = seg_loss(mask_predictions, mask.to(device))\n","              optimizer.zero_grad()\n","              loss.backward()\n","              optimizer.step()\n","              epoch_loss += loss.item()\n","              # break\n","        epoch_loss /= step+1\n","        scheduler.step(epoch_loss)\n","        val_loss = None\n","        if val_dataloader is not None: val_loss = eval(val_dataloader, sam_model, seg_loss)\n","        print(f'EPOCH: {epoch}, Loss: {epoch_loss}, Val loss {val_loss}')\n","        # wandb.log({\"epoch\": epoch, \"loss\": loss})\n","\n","        # save the latest model checkpoint\n","        # torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_'+str(epoch)+'.pth'))\n","\n","        # save the best model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_best.pth'))\n","\n","\n","\n","\n","train(**hprams)"],"metadata":{"id":"CIdDSUi-PLB0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2d Data multi node"],"metadata":{"id":"-6lc5ssnvQcK"}},{"cell_type":"code","source":["# saved 2d data\n","print(\"2d multinode\")\n","import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchvision.transforms import Compose, ToTensor, Resize\n","from torch.utils.data import Subset\n","\n","from tqdm import tqdm\n","from torchsummary import summary\n","import monai\n","from segment_anything import SamPredictor, sam_model_registry\n","from segment_anything.utils.transforms import ResizeLongestSide\n","# import wandb\n","import nibabel as nib\n","from segment_anything.utils.transforms import ResizeLongestSide\n","import torch.nn.functional as F\n","from skimage import transform, io, segmentation\n","from torch.utils.data.distributed import DistributedSampler\n","\n","\n","torch.manual_seed(2023)\n","np.random.seed(2023)\n","\n","\n","\n","def get_bounding_box(mask):\n","\n","  y_indices, x_indices = np.where(mask > 0)\n","  if len(y_indices) == 0:\n","    return [0,0,0,0]\n","  x_min, x_max = np.min(x_indices), np.max(x_indices)\n","  y_min, y_max = np.min(y_indices), np.max(y_indices)\n","  # add perturbation to bounding box coordinates\n","  H, W = mask.shape\n","  x_min = max(0, x_min - np.random.randint(0, 20))\n","  x_max = min(W, x_max + np.random.randint(0, 20))\n","  y_min = max(0, y_min - np.random.randint(0, 20))\n","  y_max = min(H, y_max + np.random.randint(0, 20))\n","  # bboxes = np.array([x_min, y_min, x_max, y_max])\n","\n","  return [x_min, y_min, x_max, y_max]\n","\n","\n","\n","\n","model_type = 'vit_h'\n","checkpoint = 'sam_vit_h_4b8939.pth'\n","sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to('cuda')\n","\n","\n","\n","embedding_path = \"/content/BraTS_data/flair/z/embeddings\"\n","ground_truth_path = \"/content/BraTS_data/mask/z\"\n","\n","data = sorted(os.listdir(embedding_path))\n","train_list = data[:int(len(data)*0.7)]\n","val_list = data[int(len(data)*0.7):]\n","\n","class BraTSDataset(Dataset):\n","\n","    def __init__(self, ground_truth_path, embedding_path, list_of_data, sam_model, device='cuda'):\n","        \"\"\"\n","        Args:\n","            embedding_path (string): Path to  images.\n","            masks_path (string): path to masks.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.ground_truth_path = ground_truth_path\n","        self.device = device\n","        self.list_of_data = list_of_data\n","        self.embedding_path = embedding_path\n","        self.sam_model = sam_model\n","\n","    def __len__(self):\n","        return len(self.list_of_data)\n","\n","    def __getitem__(self, idx):\n","\n","\n","      embedding = np.load(self.embedding_path + \"/\" + self.list_of_data[idx])\n","      embedding = torch.as_tensor(embedding, dtype=torch.float).cuda(gpu)\n","\n","\n","      #--------------------------------------------------\n","\n","      mask = np.load(self.ground_truth_path + \"/\" + self.list_of_data[idx])\n","      mask[mask>0] = 1\n","      box = get_bounding_box(mask)\n","      # mask = squarify(mask)\n","\n","\n","\n","      mask = transform.resize(\n","                      mask,\n","                      (256, 256),\n","                      order=0,\n","                      preserve_range=True,\n","                      mode=\"constant\",\n","                  )\n","\n","\n","        # class_1 = np.zeros_like(mask)\n","        # class_2 = np.zeros_like(mask)\n","        # class_3 = np.zeros_like(mask)\n","\n","        # class_1[np.where(mask == 1)] = 1\n","        # class_2[np.where(mask == 2)] = 1\n","        # class_3[np.where(mask == 4)] = 1\n","\n","        # mask = mask = np.stack((class_1, class_2, class_3), axis = -1 )\n","      mask = torch.as_tensor(mask).cuda(gpu)\n","      mask = mask[None,...]\n","\n","\n","      box = get_bounding_box(mask)\n","      # box = [0,0,mask.shape[-2],mask.shape[-1]]\n","      box_np = np.array(box)\n","      sam_trans = ResizeLongestSide(self.sam_model.image_encoder.img_size)\n","      box = sam_trans.apply_boxes(box_np, (mask.shape[-2], mask.shape[-1]))\n","      box_torch = torch.as_tensor(box, dtype=torch.float).cuda(gpu)\n","\n","\n","      with torch.no_grad():\n","      # embedding = sam_model.image_encoder(image)\n","        sparse_embeddings, dense_embeddings = self.sam_model.prompt_encoder(\n","          points=None,\n","          boxes=box_torch,\n","          masks=None,\n","      )\n","\n","\n","      sample = {'embedding': embedding,\n","                'mask': mask,\n","                'image_pe' : sam_model.prompt_encoder.get_dense_pe(),\n","                'sparse_embeddings': sparse_embeddings,\n","                'dense_embeddings': dense_embeddings,\n","                  }\n","\n","      return sample\n","\n","\n","\n","\n","def eval(val_dataloader, sam_model, seg_loss, gpu):\n","  loss = 0\n","  for step_val, data in enumerate(val_dataloader):\n","    embedding, mask, image_pe, sparse_embeddings, dense_embeddings = data[\"embedding\"], data[\"mask\"], data[\"image_pe\"], data[\"sparse_embeddings\"], data[\"dense_embeddings\"]\n","\n","\n","    with torch.no_grad():\n","      # predicted masks\n","      mask_predictions, _ = sam_model(\n","      image_embeddings= embedding.cuda(gpu), # (B, 256, 64, 64)\n","      image_pe=image_pe.cuda(gpu), # (1, 256, 64, 64)\n","      sparse_prompt_embeddings=sparse_embeddings.cuda(gpu), # (B, 2, 256)\n","      dense_prompt_embeddings=dense_embeddings.cuda(gpu), # (B, 256, 64, 64)\n","      multimask_output=False,\n","      )\n","\n","      loss += seg_loss(mask_predictions, mask.cuda(gpu))\n","  loss /= step_val+1\n","  return loss\n","\n","\n","\n","def train( gpu, args):\n","\n","    hprams = {\n","      \"num_epochs\": 15,\n","      \"best_loss\": 1e10,\n","      \"model_save_path\": \"/content\",\n","      \"lr\":1e-5,\n","      \"weight_decay\":0,\n","\n","    }\n","\n","    ngpus_per_node = torch.cuda.device_count()\n","\n","    # Get the rank of the current process\n","    SLURM_NODEID = int(os.environ.get(\"SLURM_NODEID\"))\n","\n","    # Get the local ID of the current process\n","    SLURM_LOCALID = int(os.environ.get(\"SLURM_LOCALID\"))\n","\n","  #------------------------------------\n","    rank = SLURM_NODEID * SLURM_NODEID + SLURM_LOCALID\n","    dist.init_process_group(\n","    backend=args.backend,\n","    init_method=args.init_method,\n","    world_size=args.world_size,\n","    rank=rank\n","  )\n","\n","  #-------------------------------------\n","\n","    torch.cuda.set_device(gpu)\n","\n","\n","    model_type = 'vit_h'\n","    checkpoint = 'sam_vit_h_4b8939.pth'\n","    sam_model = sam_model_registry[model_type](checkpoint=checkpoint).cuda(gpu)\n","\n","    train = BraTSDataset(ground_truth_path, embedding_path, train_list,sam_model , gpu)\n","    val = BraTSDataset(ground_truth_path, embedding_path, val_list, sam_model, gpu)\n","\n","    train_sampler = DistributedSampler(train, num_replicas=args.world_size, rank=rank, shuffle=False, drop_last=False)\n","    val_sampler = DistributedSampler(val, num_replicas=args.world_size, rank=rank, shuffle=False, drop_last=False)\n","\n","\n","    train_loader = DataLoader(train, batch_size= 8, shuffle=False, num_workers=0, sampler=train_sampler)\n","    val_loader = DataLoader(val, batch_size= 4, shuffle=False, num_workers=0, sampler=val_sampler)\n","\n","    model_save_path = hprams['model_save_path']\n","    num_epochs = hprams['num_epochs']\n","    best_loss = hprams['best_loss']\n","    lr = hprams['lr']\n","    weight_decay = hprams['weight_decay']\n","\n","\n","    os.makedirs(model_save_path, exist_ok=True)\n","\n","    model = sam_model.mask_decoder\n","    model.cuda(gpu)\n","\n","\n","    model.train()\n","    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, factor=0.25,  patience=5)\n","    seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean').cuda(gpu)\n","\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        # train\n","        for step, data in enumerate(tqdm(train_dataloader)):\n","            embedding, mask, image_pe, sparse_embeddings, dense_embeddings = data[\"embedding\"], data[\"mask\"], data[\"image_pe\"], data[\"sparse_embeddings\"], data[\"dense_embeddings\"]\n","\n","            # with torch.no_grad():\n","\n","            #   sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n","            #       points=None,\n","            #       boxes=box,\n","            #       masks=None,\n","            #   )\n","\n","            # predicted masks\n","            mask_predictions, _ = model(\n","              image_embeddings= embedding.cuda(gpu), # (B, 256, 64, 64)\n","              image_pe=image_pe.cuda(gpu), # (1, 256, 64, 64)\n","              sparse_prompt_embeddings=sparse_embeddings.cuda(gpu), # (B, 2, 256)\n","              dense_prompt_embeddings=dense_embeddings.cuda(gpu), # (B, 256, 64, 64)\n","              multimask_output=False,\n","            )\n","\n","            loss = seg_loss(mask_predictions, mask.cuda(gpu))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","            # break\n","        epoch_loss /= step+1\n","        scheduler.step(epoch_loss)\n","        val_loss = None\n","        if val_dataloader is not None: val_loss = eval(val_dataloader, model, seg_loss, gpu)\n","        print(f'EPOCH: {epoch}, Loss: {epoch_loss}, Val loss {val_loss}')\n","        # wandb.log({\"epoch\": epoch, \"loss\": loss})\n","\n","        # save the latest model checkpoint\n","        # torch.save(sam_model.state_dict(), os.path.join(model_save_path, 'sam_model_'+str(epoch)+'.pth'))\n","\n","        # save the best model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save(model.module.state_dict(), os.path.join(model_save_path, 'sam_model_best.pth'))\n","\n","\n","\n","def main():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('-n', '--nodes', default=1,\n","                        type=int, metavar='N')\n","    parser.add_argument('-g', '--gpus', default=1, type=int,\n","                        help='number of gpus per node')\n","    parser.add_argument('-nr', '--nr', default=0, type=int,\n","                        help='ranking within the nodes')\n","\n","\n","\n","    args = parser.parse_args()\n","\n","    args.world_size = args.gpus * args.nodes\n","    os.environ['MASTER_ADDR'] = ''\n","    os.environ['MASTER_PORT'] = '8888'\n","    mp.spawn(train, nprocs=args.gpus, args=(args,))\n","\n","\n","if __name__ == \"__main__\":\n","  main()"],"metadata":{"id":"q2j99R6pvQsp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def a():\n","  def b():\n","    print(\"b\")\n","\n","\n","a.b()"],"metadata":{"id":"daCQME_66589","colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"status":"error","timestamp":1689768604611,"user_tz":-180,"elapsed":282,"user":{"displayName":"Mohannad Barakat","userId":"02596848204094181562"}},"outputId":"cf193a0b-acda-4f85-df07-4cf75182fcd1"},"execution_count":1,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-75688886d872>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'b'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UWi1NkxwDqpA"},"execution_count":null,"outputs":[]}]}