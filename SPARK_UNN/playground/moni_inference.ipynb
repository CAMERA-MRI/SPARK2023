{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import torchio as tio\n",
    "import logging\n",
    "\n",
    "# import torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Import MONAI libraries                <--- CLEAN UP THESE IMPORTS ONCE WE KNOW WHAT libraries are used\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "from monai.data import ArrayDataset, decollate_batch, DataLoader\n",
    "from monai.handlers import (\n",
    "    CheckpointLoader,\n",
    "    IgniteMetric,\n",
    "    MeanDice,\n",
    "    StatsHandler,\n",
    "    TensorBoardImageHandler,\n",
    "    TensorBoardStatsHandler,\n",
    ")\n",
    "from monai.metrics import DiceMetric, LossMetric, HausdorffDistanceMetric\n",
    "from monai.losses import DiceLoss, DiceFocalLoss\n",
    "from monai.networks import nets as monNets\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirst,\n",
    "    AsDiscrete,\n",
    "    Compose\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.utils import first\n",
    "from monai.utils.misc import set_determinism\n",
    "\n",
    "# Other imports (unsure)\n",
    "# import ignite\n",
    "import nibabel\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import torch.utils.data as data_utils\n",
    "import json\n",
    "from subprocess import call\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from utils.utils import get_main_args\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Given a set of images and corresponding labels (i.e, will give it all training images + labels, and same for val and test)\n",
    "    folder structure: subjectID/subjectID-stk.npy, -lbl.npy (i.e. contains 2 files)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, data_folders, transform=None, SSAtransform=None):\n",
    "            self.data_folders = data_folders                            # path for each data folder in the set\n",
    "            self.transform = transform\n",
    "            self.SSAtransform = SSAtransform\n",
    "            self.imgs = []                                              # store images to load (paths)\n",
    "            self.lbls = []                                              # store corresponding labels (paths)\n",
    "            for img_folder in self.data_folders:                        # run through each subjectID folder\n",
    "                folder_path = os.path.join(data_dir, img_folder)                                                            \n",
    "                self.SSA = True if 'SSA' in img_folder else False       # check if current file is from SSA dataset\n",
    "                for file in os.listdir(folder_path):                    # check folder contents\n",
    "                    if os.path.isfile(os.path.join(folder_path, file)):\n",
    "                        if file.endswith(\"-lbl.npy\"):\n",
    "                            self.lbls.append(os.path.join(folder_path, file))   # Save segmentation mask (file path)\n",
    "                            self.mode = \"labels\"\n",
    "                        elif file.endswith(\"-stk.npy\"):\n",
    "                            self.imgs.append(os.path.join(folder_path, file))   # Save image (file path)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the amount of images in this set\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        name = os.path.dirname(self.imgs[idx])\n",
    "        # Load files\n",
    "        if self.mode == \"labels\":\n",
    "            mask = np.load(self.lbls[idx])\n",
    "            mask = torch.from_numpy(mask) # 240, 240, 155\n",
    "\n",
    "        # print(self.imgs[idx] )\n",
    "        # print(\"========================\")\n",
    "        # print(self.lbls[idx] )\n",
    "        # print(\"========================\")           \n",
    "\n",
    "        if self.transform is not None: # Apply general transformations\n",
    "        # transforms such as crop, flip, rotate etc will be applied to both the image and the mask\n",
    "            if self.mode == \"labels\":\n",
    "                subject = tio.Subject(\n",
    "                    image=tio.ScalarImage(tensor=image),\n",
    "                    mask=tio.LabelMap(tensor=mask)\n",
    "                    )\n",
    "                tranformed_subject = self.transform(subject)\n",
    "                # Apply transformation to GLI data to reduce quality (creating fake SSA data)\n",
    "                if self.SSA == False and self.SSAtransform is not None:\n",
    "                    tranformed_subject = self.SSAtransform(tranformed_subject)\n",
    "            \n",
    "                print(\"Tranformed_subject: \", tranformed_subject)\n",
    "                image = tranformed_subject[\"image\"].data\n",
    "                mask = tranformed_subject[\"mask\"].data\n",
    "                return image, mask, self.imgs[idx]\n",
    "            else:\n",
    "                subject = tio.Subject(\n",
    "                    image=tio.ScalarImage(tensor=image),\n",
    "                    )\n",
    "                tranformed_subject = self.transform(subject)           \n",
    "                print(\"Tranformed_subject: \", tranformed_subject)\n",
    "                image = tranformed_subject[\"image\"].data\n",
    "                return image, self.imgs[idx]\n",
    "\n",
    "        return image, mask, self.imgs[idx]\n",
    "    \n",
    "    def get_paths(self):\n",
    "        return self.img_pth, self.seg_pth\n",
    "    \n",
    "    def get_subj_info(self):\n",
    "        return self.subj_dir_pths, self.subj_dirs\n",
    "        #, self.SSA\n",
    "    \n",
    "    def get_transforms(self):\n",
    "        return self.transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_transforms(n_channels):\n",
    "    # Initialise data transforms\n",
    "    data_transforms = {\n",
    "        'train': tio.Compose([\n",
    "            tio.CropOrPad((192, 192, 124)),\n",
    "            tio.OneOf([\n",
    "                tio.Compose([\n",
    "                    tio.RandomFlip(axes=0, p=0.3),\n",
    "                    tio.RandomFlip(axes=1, p=0.3),\n",
    "                    tio.RandomFlip(axes=2, p=0.3)]),\n",
    "                tio.RandomAffine(degrees=15,p=0.3)\n",
    "            ], p=0.8),\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ]),\n",
    "        'fakeSSA': tio.OneOf({\n",
    "            tio.OneOf({\n",
    "                tio.Compose([\n",
    "                    tio.Resample((1.2, 1.2, 6), scalars_only=True),\n",
    "                    tio.Resample(1)\n",
    "                ]):0.50,\n",
    "                tio.Compose([\n",
    "                    tio.RandomAnisotropy(axes=(1, 2), downsampling=(1.2), scalars_only=True),\n",
    "                    tio.RandomAnisotropy(axes=0, downsampling=(6), scalars_only=True)\n",
    "                ]):0.5,                \n",
    "            },p=0.80),\n",
    "            tio.Compose([            \n",
    "                tio.OneOf({\n",
    "                    tio.RandomBlur(std=(0.5, 1.5)) : 0.3,\n",
    "                    tio.RandomNoise(mean=3, std=(0, 0.33)) : 0.7\n",
    "                },p=0.50),\n",
    "                tio.OneOf({\n",
    "                    tio.RandomMotion(num_transforms=3, image_interpolation='nearest') : 0.5,\n",
    "                    tio.RandomBiasField(coefficients=1) : 0.2,\n",
    "                    tio.RandomGhosting(intensity=1.5) : 0.3\n",
    "                }, p=0.50)\n",
    "            ])\n",
    "        }, p=0.8), # randomly apply ONE of these given transforms with prob 0.5 \n",
    "        'val': tio.Compose([\n",
    "            tio.CropOrPad((192, 192, 124)),\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ]),\n",
    "        'test' : tio.Compose([\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    return data_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args, data_transforms):\n",
    "\n",
    "    '''\n",
    "    This function is called during training after define_transforms(n_channels)\n",
    "\n",
    "    It takes as input\n",
    "        args: argparsers from the utils script \n",
    "            args.seed\n",
    "            args.data_used: 'all', 'GLI', 'SSA'\n",
    "        data_transforms: a dictionary of transformations to apply to the data during training\n",
    "\n",
    "    Returns dataloaders ready to be fed into model\n",
    "    '''\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Set a seed for reproducibility if you want the same split - optional\n",
    "    if args.seed != None:\n",
    "        seed=args.seed\n",
    "        logger.info(f\"Seed set to {seed}.\")\n",
    "    else:\n",
    "        seed=None\n",
    "        logger.info(\"No seed has been set\")\n",
    "    \n",
    "    # Locate data based on which dataset is being used\n",
    "    if args.data_used == 'all':\n",
    "        data_folders = glob.glob(os.path.join(args.data, \"BraTS*\"))\n",
    "    elif args.data_used == \"GLI\":\n",
    "        data_folders = [folder for folder in os.listdir(args.data) if 'GLI' in folder]\n",
    "    elif args.data_used == 'SSA':\n",
    "        data_folders = [folder for folder in os.listdir(args.data) if 'SSA' in folder]\n",
    "\n",
    "    # Split data files\n",
    "    train_files, val_files = split_data(data_folders, seed) \n",
    "    logger.info(f\"Number of training files: {len(train_files)}\\nNumber of validation files: {len(val_files)}\")\n",
    "    \n",
    "    image_datasets = {\n",
    "        'train': MRIDataset(args.data, train_files, transform=data_transforms['train']),\n",
    "        'val': MRIDataset(args.data, val_files, transform=data_transforms['val']),\n",
    "    }\n",
    "\n",
    "    # Create dataloaders\n",
    "    # can set num_workers for running sub-processes\n",
    "    dataloaders = {\n",
    "        'train': data_utils.DataLoader(image_datasets['train'], batch_size=args.batch_size, shuffle=True, drop_last=True),\n",
    "        'val': data_utils.DataLoader(image_datasets['val'], batch_size=args.val_batch_size, shuffle=True)\n",
    "        # 'test': data_utils.DataLoader(image_datasets['test'], batch_size=args.val_batch_size, shuffle=True)\n",
    "    }\n",
    "\n",
    "    # Save data split\n",
    "    splitData = {\n",
    "        'subjsTr' : train_files,\n",
    "        'subjsVal' : val_files,\n",
    "        # 'subjsTest' : test_files    \n",
    "    }\n",
    "    with open(args.data + str(args.data_used) + \".json\", \"w\") as file:\n",
    "        json.dump(splitData, file)\n",
    "\n",
    "    return dataloaders\n",
    "\n",
    "def split_data(data_folders, seed):\n",
    "    '''\n",
    "    Function to split dataset into train/val/test splits, given all avilable data.\n",
    "    Input:\n",
    "        list of paths to numpy files\n",
    "    Returns:\n",
    "        lists for each train and val/test sets, where each list contains the file names to be used in the set\n",
    "    '''\n",
    "    #-----------------------------\n",
    "    # originally we split as 3: train-test-val train (70), val (15), test (15):\n",
    "        # train_files, test_files = train_test_split(data_folders, test_size=0.7, random_state=seed)\n",
    "        # val_files, test_files = train_test_split(test_files, test_size=0.5, random_state=seed)\n",
    "\n",
    "    #-----------------------------\n",
    "    # training loop split is train-val (70-30)\n",
    "    train_files, val_files = train_test_split(data_folders, test_size=0.7, random_state=seed)\n",
    "\n",
    "    # ??? validation/testing???\n",
    "\n",
    "    return train_files, val_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup transforms, dataset\"\"\"\n",
    "def define_dataloaders(n_channels):\n",
    "    # Define transforms\n",
    "    data_transform = define_transforms(n_channels)\n",
    "    # Load data\n",
    "    dataloaders = load_data(args, data_transform)                      # this also saves a json splitData\n",
    "    # train_loader, val_loader = dataloaders['train'], dataloaders['val']\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define model architecture:\n",
    "        Done before data loader so that transforms has n_channels for EnsureShapeMultiple\n",
    "\"\"\"\n",
    "def define_model(checkpoint=None):\n",
    "    model=UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        # channels=(32, 64, 128, 256, 320, 320), #nnunet channels, depth 6\n",
    "        # channels=(64, 96, 128, 192, 256, 384, 512), # optinet, depth 7\n",
    "        strides=(2, 2, 2, 2), # length should = len(channels) - 1\n",
    "        # kernel_size=,\n",
    "        # num_res_units=,\n",
    "        # dropout=0.0,\n",
    "        )\n",
    "    n_channels = len(model.channels)\n",
    "    print(f\"Number of channels: {n_channels}\")\n",
    "\n",
    "    if checkpoint != None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=torch.device('cpu')))\n",
    "\n",
    "    return model, n_channels\n",
    "\n",
    "\"\"\"\n",
    "Setup validation stuff\n",
    "    metrics\n",
    "    post trans ???????\n",
    "    define inference\n",
    "\"\"\"\n",
    "def val_params():\n",
    "    VAL_AMP = True\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=True, num_classes=4)\n",
    "    dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\", get_not_nans=True, num_classes=4)\n",
    "    post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "    return VAL_AMP, dice_metric, dice_metric_batch, post_trans\n",
    "\n",
    "# define inference method\n",
    "def inference(VAL_AMP, model, input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=None, ## best to leave as None, computes itself (else NEEDS TO BE BASED ON LAYER IN, OUT E.G., Conv3d(4, 16; Conv3d(16, 32; etc)\n",
    "            # roi_size=(128, 128, 64),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "            mode='gaussian'\n",
    "        )\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (model): Sequential(\n",
       "    (0): Convolution(\n",
       "      (conv): Conv3d(4, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (adn): ADN(\n",
       "        (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (D): Dropout(p=0.0, inplace=False)\n",
       "        (A): PReLU(num_parameters=1)\n",
       "      )\n",
       "    )\n",
       "    (1): SkipConnection(\n",
       "      (submodule): Sequential(\n",
       "        (0): Convolution(\n",
       "          (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "          (adn): ADN(\n",
       "            (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (D): Dropout(p=0.0, inplace=False)\n",
       "            (A): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "        (1): SkipConnection(\n",
       "          (submodule): Sequential(\n",
       "            (0): Convolution(\n",
       "              (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                (D): Dropout(p=0.0, inplace=False)\n",
       "                (A): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "            (1): SkipConnection(\n",
       "              (submodule): Sequential(\n",
       "                (0): Convolution(\n",
       "                  (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "                  (adn): ADN(\n",
       "                    (N): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                    (D): Dropout(p=0.0, inplace=False)\n",
       "                    (A): PReLU(num_parameters=1)\n",
       "                  )\n",
       "                )\n",
       "                (1): SkipConnection(\n",
       "                  (submodule): Convolution(\n",
       "                    (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "                    (adn): ADN(\n",
       "                      (N): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                      (D): Dropout(p=0.0, inplace=False)\n",
       "                      (A): PReLU(num_parameters=1)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (2): Convolution(\n",
       "                  (conv): ConvTranspose3d(384, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
       "                  (adn): ADN(\n",
       "                    (N): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                    (D): Dropout(p=0.0, inplace=False)\n",
       "                    (A): PReLU(num_parameters=1)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): Convolution(\n",
       "              (conv): ConvTranspose3d(128, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
       "              (adn): ADN(\n",
       "                (N): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "                (D): Dropout(p=0.0, inplace=False)\n",
       "                (A): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Convolution(\n",
       "          (conv): ConvTranspose3d(64, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
       "          (adn): ADN(\n",
       "            (N): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (D): Dropout(p=0.0, inplace=False)\n",
       "            (A): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Convolution(\n",
       "      (conv): ConvTranspose3d(32, 4, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"General Setup: \n",
    "    logging,\n",
    "    utils.args \n",
    "    seed,\n",
    "    cuda, \n",
    "    root dir\"\"\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# args = get_main_args()\n",
    "seed = 42\n",
    "set_determinism(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# root_dir = args.data\n",
    "# results_dir = args.results\n",
    "\n",
    "model, n_channels = define_model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONT RUN THIS FOR TESTING INFERENCE (TRAIN STUFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Potentially useful functions for model tracking and checkpoint loading\n",
    "\"\"\"\n",
    "# def save_checkpoint(model, epoch, best_acc=0, dir_add=results_dir, args=args):\n",
    "#     filename=f\"chkpt_{args.run_name}_{epoch}_{best_acc}.pt\"\n",
    "#     state_dict = model.state_dict()\n",
    "#     save_dict = {\"epoch\": epoch, \"best_acc\": best_acc, \"state_dict\": state_dict}\n",
    "#     filename = os.path.join(dir_add, filename)\n",
    "#     torch.save(save_dict, filename)\n",
    "#     print(\"\\nSaving checkpoint\", filename)\n",
    "\n",
    "\"\"\"Setup transforms, dataset\"\"\"\n",
    "def define_dataloaders(n_channels):\n",
    "    # Define transforms\n",
    "    data_transform = define_transforms(n_channels)\n",
    "    # Load data\n",
    "    dataloaders = load_data(args, data_transform)                      # this also saves a json splitData\n",
    "    # train_loader, val_loader = dataloaders['train'], dataloaders['val']\n",
    "    return dataloaders\n",
    "\n",
    "\"\"\"Create Model Params:\n",
    "    optimiser\n",
    "    loss fn\n",
    "    lr\n",
    "\"\"\"\n",
    "def model_params(args, model):\n",
    "    # Define optimiser\n",
    "    if args.optimiser == \"adam\":\n",
    "        optimiser = torch.optim.Adam(params=model.parameters(), lr=args.learning_rate)\n",
    "        print(\"Adam optimizer set\")\n",
    "    elif args.optimiser == \"sgd\":\n",
    "        optimiser = torch.optim.SGD(params=model.parameters())\n",
    "        print(\"SGD optimizer set\")\n",
    "    elif args.optimiser == \"novo\":\n",
    "        optimiser = monai.optimizers.Novograd(params=model.parameters(), lr=args.learning_rate)\n",
    "    else:\n",
    "        print(\"Error, no optimiser provided\")\n",
    "\n",
    "    # Define loss function\n",
    "    if args.criterion == \"ce\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(\"Cross Entropy Loss set\")\n",
    "    elif args.criterion == \"dice\":\n",
    "        criterion = DiceFocalLoss(squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "        print(\"Focal-Dice Loss set\")\n",
    "    else:\n",
    "        print(\"Error, no loss fn provided\")\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=args.epochs)\n",
    "    \n",
    "    return optimiser, criterion, lr_scheduler\n",
    "\n",
    "\"\"\"\n",
    "Define training loop\n",
    "    initialise empty lists for val\n",
    "    Add GradScalar which uses automatic mixed precision to accelerate training\n",
    "    forward and backward passes\n",
    "    validate training epoch\n",
    "\n",
    "\"\"\"\n",
    "def train(args, model, device, train_loader, val_loader, optimiser, criterion, lr_scheduler):\n",
    "\n",
    "    VAL_AMP, dice_metric, dice_metric_batch, post_trans = val_params()\n",
    "\n",
    "    # Train model --> see MONAI notebook examples\n",
    "    val_interval = 1\n",
    "    epoch_loss_list = []\n",
    "    val_epoch_loss_list = []\n",
    "\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    best_metrics_epochs_and_time = [[], [], []]\n",
    "\n",
    "    metric_values = []\n",
    "    metric_values_0 = []\n",
    "    metric_values_1 = []\n",
    "    metric_values_2 = []\n",
    "    metric_values_3 = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        epoch_start = time.time()\n",
    "        # print(\"-\" * 10)\n",
    "        # print(f\"epoch {epoch + 1}/{args.epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), dynamic_ncols=True)\n",
    "        progress_bar.set_description(f\"Training Epoch {epoch}\")\n",
    "\n",
    "        # for step, batch in progress_bar:\n",
    "        for step, batch_data in progress_bar:\n",
    "            step_start = time.time()\n",
    "            inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "            optimiser.zero_grad()\n",
    "        \n",
    "            with autocast(): # cast tensor to smaller memory footprint to avoid OOM\n",
    "                \"\"\" FOR USE WITH A DIFFUSION MODEL ONLY\n",
    "                # Generate random noise\n",
    "                noise = torch.randn_like(images).to(device)\n",
    "                Create timesteps\n",
    "                timesteps = torch.randint(\n",
    "                    0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                ).long()\n",
    "                # Get model prediction\n",
    "                noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "                \"\"\"\n",
    "\n",
    "                # print(inputs.shape)\n",
    "                outputs = model(inputs)\n",
    "                # print(outputs.shape)\n",
    "                loss = criterion.forward(outputs, labels)\n",
    "            \n",
    "            # Calculate Loss and Update optimiser using scalar\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimiser)\n",
    "            scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"bat_train_loss\" : loss.item(), \"Ave_train_loss\" : epoch_loss/(step + 1)})\n",
    "            \n",
    "            print(\n",
    "                f\"\\n{step}/{len(train_loader.dataset)//train_loader.batch_size}\"\n",
    "                f\",     Batch train_loss: {loss.item():.4f}\"\n",
    "                f\",     Step time: {(time.time() - step_start):.4f}\"\n",
    "            )\n",
    "            epoch_loss2 = epoch_loss/(step+1)\n",
    "            lr_scheduler.step()\n",
    "        epoch_loss_list.append(epoch_loss2)\n",
    "        print(f\"\\nEpoch {epoch} average loss: {epoch_loss2:.4f}\")\n",
    "        \n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            val_epoch_loss = 0\n",
    "            progress_bar = tqdm(enumerate(val_loader), total=len(val_loader),dynamic_ncols=True)\n",
    "            progress_bar.set_description(f\"Val_train Epoch {epoch}\")\n",
    "\n",
    "            for step, batch in enumerate(val_loader):\n",
    "                val_inputs, val_labels = batch[0].to(device), batch[1].to(device)\n",
    "                \"\"\" FOR USE WITH A DIFFUSION MODEL ONLY\n",
    "                timesteps = torch.randint(\n",
    "                    0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                ).long()\n",
    "\n",
    "                # Get model prediction\n",
    "                noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "                val_loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "                \"\"\"\n",
    "                with torch.no_grad():\n",
    "                    val_outputs = inference(VAL_AMP, model, val_inputs)\n",
    "                    val_loss = criterion.forward(val_outputs, val_labels)\n",
    "                    \n",
    "                    val_labels_list = decollate_batch(val_labels)\n",
    "                    val_outputs_convert = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                    dice_metric(y_pred=val_outputs_convert, y=val_labels_list)\n",
    "                    dice_metric_batch(y_pred=val_outputs_convert, y=val_labels_list)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                progress_bar.set_postfix({\"Val_loss\": val_epoch_loss / (step + 1)})\n",
    "            val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
    "            \n",
    "            metric = dice_metric.aggregate()[0].item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            # print(metric)\n",
    "            # print(metric_batch)\n",
    "\n",
    "            metric_0 = metric_batch[0][0].item()\n",
    "            metric_values_0.append(metric_0)\n",
    "\n",
    "            metric_1 = metric_batch[0][1].item()\n",
    "            metric_values_1.append(metric_1)\n",
    "\n",
    "            metric_2 = metric_batch[0][2].item()\n",
    "            metric_values_2.append(metric_2)\n",
    "\n",
    "            metric_3 = metric_batch[0][3].item()\n",
    "            metric_values_3.append(metric_3)\n",
    "\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                save_checkpoint(\n",
    "                        model,\n",
    "                        epoch,\n",
    "                        best_acc=best_metric,\n",
    "                    )\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(args.result, f\"best_metric_model_{args.run_name}.pth\"),\n",
    "                )\n",
    "                print(\"\\nsaved new best metric model\")\n",
    "            print(\n",
    "                f\"\\ncurrent epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\"\\nMean Dice per Region is: label 1: {metric_1:.4f};  label 2: {metric_2:.4f} label 3: {metric_3:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "        print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "    total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = define_dataloaders(n_channels)\n",
    "train_loader, val_loader = dataloaders['train'], dataloaders['val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser, criterion, lr_scheduler = model_params(args, model)\n",
    "\n",
    "# TRAIN MODEL\n",
    "train(args, model, device, train_loader, val_loader, optimiser, criterion, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n",
    "y = metric_values_tc\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n",
    "y = metric_values_wt\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n",
    "y = metric_values_et\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN FROM HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File paths for Alex's local system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/guest187/Data/val_SSA/BraTS-SSA-00139-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00227-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00126-000', '/scratch/guest187/Data/val_SSA/dataset.json', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00192-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00143-000', '/scratch/guest187/Data/val_SSA/images', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00169-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00129-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00180-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00158-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00132-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00198-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00210-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00188-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00218-000', '/scratch/guest187/Data/val_SSA/BraTS-SSA-00148-000']\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/scratch/guest187/Data/val_SSA/dataset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load validation data to dataloader\u001b[39;00m\n\u001b[1;32m     12\u001b[0m data_transforms \u001b[38;5;241m=\u001b[39m define_transforms(n_channels)\n\u001b[0;32m---> 13\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMRIDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_transforms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Validation parameters\u001b[39;00m\n\u001b[1;32m     16\u001b[0m VAL_AMP, dice_metric, dice_metric_batch, post_transforms \u001b[38;5;241m=\u001b[39m val_params()\n",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m, in \u001b[0;36mMRIDataset.__init__\u001b[0;34m(self, data_dir, data_folders, transform, SSAtransform)\u001b[0m\n\u001b[1;32m     14\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, img_folder)                                                            \n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSSA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSSA\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m img_folder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m       \u001b[38;5;66;03m# check if current file is from SSA dataset\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m:                    \u001b[38;5;66;03m# check folder contents\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file)):\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-lbl.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/scratch/guest187/Data/val_SSA/dataset.json'"
     ]
    }
   ],
   "source": [
    "validation_dir= '/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/val_SSA'\n",
    "validation_files = [os.path.join(validation_dir, file) for file in os.listdir(validation_dir)]\n",
    "\n",
    "# checkpoint to test: /scratch/guest187/Data/train_all/results/test_run/best_metric_model.pth \n",
    "checkpoint = '/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/best_metric_model_fullTest.pth'\n",
    "\n",
    "# Load validation data to dataloader\n",
    "data_transforms = define_transforms(n_channels)\n",
    "validation_dataset = MRIDataset(validation_dir, validation_files, transform=data_transforms['val'])\n",
    "\n",
    "# Validation parameters\n",
    "VAL_AMP, dice_metric, dice_metric_batch, post_transforms = val_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "tranformed_subject:  Subject(Keys: ('image',); images: 1)\n",
      "torch.Size([4, 192, 192, 128])\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_dataset))\n",
    "img, image_path = validation_dataset[0]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels: 5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for UNet:\n\tMissing key(s) in state_dict: \"model.0.conv.weight\", \"model.0.conv.bias\", \"model.0.adn.A.weight\", \"model.1.submodule.0.conv.weight\", \"model.1.submodule.0.conv.bias\", \"model.1.submodule.0.adn.A.weight\", \"model.1.submodule.1.submodule.0.conv.weight\", \"model.1.submodule.1.submodule.0.conv.bias\", \"model.1.submodule.1.submodule.0.adn.A.weight\", \"model.1.submodule.1.submodule.1.submodule.0.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.0.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.0.adn.A.weight\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.adn.A.weight\", \"model.1.submodule.1.submodule.1.submodule.2.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.2.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.2.adn.A.weight\", \"model.1.submodule.1.submodule.2.conv.weight\", \"model.1.submodule.1.submodule.2.conv.bias\", \"model.1.submodule.1.submodule.2.adn.A.weight\", \"model.1.submodule.2.conv.weight\", \"model.1.submodule.2.conv.bias\", \"model.1.submodule.2.adn.A.weight\", \"model.2.conv.weight\", \"model.2.conv.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"global_step\", \"pytorch-lightning_version\", \"state_dict\", \"loops\", \"callbacks\", \"optimizer_states\", \"lr_schedulers\", \"NativeMixedPrecisionPlugin\", \"hparams_name\", \"hyper_parameters\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model, n_channels \u001b[38;5;241m=\u001b[39m \u001b[43mdefine_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[23], line 21\u001b[0m, in \u001b[0;36mdefine_model\u001b[0;34m(checkpoint)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of channels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_channels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, n_channels\n",
      "File \u001b[0;32m~/hackathon/lib/python3.9/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UNet:\n\tMissing key(s) in state_dict: \"model.0.conv.weight\", \"model.0.conv.bias\", \"model.0.adn.A.weight\", \"model.1.submodule.0.conv.weight\", \"model.1.submodule.0.conv.bias\", \"model.1.submodule.0.adn.A.weight\", \"model.1.submodule.1.submodule.0.conv.weight\", \"model.1.submodule.1.submodule.0.conv.bias\", \"model.1.submodule.1.submodule.0.adn.A.weight\", \"model.1.submodule.1.submodule.1.submodule.0.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.0.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.0.adn.A.weight\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.1.submodule.adn.A.weight\", \"model.1.submodule.1.submodule.1.submodule.2.conv.weight\", \"model.1.submodule.1.submodule.1.submodule.2.conv.bias\", \"model.1.submodule.1.submodule.1.submodule.2.adn.A.weight\", \"model.1.submodule.1.submodule.2.conv.weight\", \"model.1.submodule.1.submodule.2.conv.bias\", \"model.1.submodule.1.submodule.2.adn.A.weight\", \"model.1.submodule.2.conv.weight\", \"model.1.submodule.2.conv.bias\", \"model.1.submodule.2.adn.A.weight\", \"model.2.conv.weight\", \"model.2.conv.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"global_step\", \"pytorch-lightning_version\", \"state_dict\", \"loops\", \"callbacks\", \"optimizer_states\", \"lr_schedulers\", \"NativeMixedPrecisionPlugin\", \"hparams_name\", \"hyper_parameters\". "
     ]
    }
   ],
   "source": [
    "model, n_channels = define_model(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_input, _ = validation_dataset[0]\n",
    "    print(val_input.shape)\n",
    "    # roi_size = (128, 128, 64)\n",
    "    # sw_batch_size = 4\n",
    "    val_output = inference(VAL_AMP, model, val_input)\n",
    "    val_output = post_transforms(val_output[0])\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(validation_dataset[6][\"image\"][i, :, :, 70].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(validation_dataset[6][\"label\"][i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
