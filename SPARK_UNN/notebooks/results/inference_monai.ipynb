{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os \n",
    "class Args(argparse.Namespace):\n",
    "    # data=\"/scratch/guest187/Data/val_SSA/monai\"\n",
    "    # data = \"/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/val_SSA/monai/\"\n",
    "    # data=\"C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\images\\\\\"\n",
    "    data='C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\Data\\\\SSA_Val'\n",
    "    preproc_set=\"val\"\n",
    "    data_used=\"SSA\"\n",
    "    # results='/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/val_SSA/results/'\n",
    "    # results='/scratch/guest187/Data/val_SSA/results/monai_test/'\n",
    "    results='C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\train_all\\\\'\n",
    "    optimiser=\"adam\"\n",
    "    criterion=\"dice\"\n",
    "    exec_mode=\"predict\"\n",
    "    seed=42\n",
    "    batch_size=4\n",
    "    val_batch_size=2\n",
    "    # ckpt_path='/scratch/guest187/Data/train_all/results/test_fullRunThrough/best_metric_model_fullTest.pth'\n",
    "    # ckpt_path='/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/best_metric_model_fullTest.pth'\n",
    "    ckpt_path='C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\CC\\\\Backup_2407\\\\Results\\\\train_all_monai\\\\test_fullRunThrough\\\\best_metric_model_fullTest.pth'\n",
    "    model=\"unet\"\n",
    "args=Args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB!!!! IF CALLING THE 'FROM monai_functions IMPORT (list of fx)': MUST COPY ABOVE ARGS INTO THE MONAI FX PY SCRIPT AND COMMENT OUT DL.GET ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "# sys.path.append('/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/scripts')\n",
    "sys.path.append('C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\UNN_BraTS23\\\\scripts')\n",
    "import subprocess\n",
    "try:\n",
    "    import os\n",
    "    import numpy as np \n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    import torchio as tio\n",
    "    import nibabel as nib\n",
    "    from glob import glob\n",
    "    from subprocess import call\n",
    "    from scipy.ndimage import label\n",
    "    from monai.inferers import sliding_window_inference\n",
    "    import modelZoo_monai as mZoo\n",
    "    import data_loader as dl\n",
    "    from monai_functions import (\n",
    "        save_checkpoint,\n",
    "        define_model,\n",
    "        define_dataloaders,\n",
    "        model_params,\n",
    "        val_params,\n",
    "        train)\n",
    "    \n",
    "except ModuleNotFoundError as e:\n",
    "    package = str(e).split(\"'\")[0]\n",
    "    subprocess.run(['pip', 'install', package])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inference method\n",
    "def inference(VAL_AMP, model, input):\n",
    "    def _compute(input):\n",
    "        '''\n",
    "        roi_size â€“ the spatial window size for inferences. \n",
    "        When its components have None or non-positives, the corresponding inputs dimension will be used. \n",
    "        if the components of the roi_size are non-positive values, the transform will use the corresponding components of img size.\n",
    "        For example, roi_size=(32, -1) will be adapted to (32, 64) if the second spatial dimension size of img is 64\n",
    "        '''\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=None,\n",
    "            sw_batch_size=4, #sw_batch_size denotes the max number of windows per network inference iteration, not the batch size of inputs.\n",
    "            predictor=model,\n",
    "            overlap=0.2,\n",
    "            mode='constant'\n",
    "        )\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "\n",
    "def infer():\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            input = batch[0]\n",
    "            # case = str(batch[1]).split('/')[-1].replace(\"-stk.npy',)\", \"\")\n",
    "            case = str(batch[1]).split('\\\\')[-1].replace(\"-stk.npy',)\", \"\")\n",
    "            print(f\"Working on subject: {case}; shape size is: {input.shape}\")\n",
    "            #Run inference\n",
    "            output = inference(VAL_AMP, model, input)\n",
    "            print(f\"Output before post_transforms is: {output.shape}\")\n",
    "            val_output = post_transforms(output[0])\n",
    "            print(f\"Output after post_transforms is: {val_output.shape}\")\n",
    "\n",
    "            #Attempt to convert and save to numpy for post processing\n",
    "            val_output2 = val_output.cpu().numpy()\n",
    "            val_out_pth = os.path.join(args.results, f\"{case}_preds.npy\")\n",
    "            try:\n",
    "                print(f\"trying to save to numpy file, with shape: {val_output2.shape}\")\n",
    "                np.save(val_out_pth, val_output2)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while saving the file: {e}\")\n",
    "\n",
    "# Save to nifti\n",
    "def save_nifti(final_np, img, outfile):\n",
    "    '''\n",
    "    Save numpy to NIfTI format\n",
    "    input numpy must have 3 dimensions only\n",
    "    '''\n",
    "    \n",
    "    img = nib.load(img) ## load original image from path\n",
    "    nib.save(\n",
    "        nib.Nifti1Image(final_np, img.affine, header=img.header),\n",
    "        outfile\n",
    "    )\n",
    "\n",
    "def save_final_preds(pred, img, outfile, fname):\n",
    "    #convert tensor to tio subject for padding and affine alignment\n",
    "    predT = torch.from_numpy(pred).unsqueeze(0)\n",
    "    subject = tio.Subject(\n",
    "            image=tio.ScalarImage(tensor=predT),\n",
    "            name=fname\n",
    "            )\n",
    "    tranformed_subject = tio.CropOrPad((240, 240, 155))\n",
    "    tranformed_subject = tranformed_subject(subject)\n",
    "    pred_final = tranformed_subject[\"image\"].data.numpy().squeeze(0)\n",
    "    print(f\"Shape chanes: initial = {pred.shape}, tensor = {predT.shape}, and final npy = {pred_final.shape}\")\n",
    "\n",
    "    save_nifti(pred_final, img, outfile)\n",
    "\n",
    "\n",
    "#------------ OPTINET -----##\n",
    "def to_lbl(pred):\n",
    "    print(\"in to_lbl: \", pred.shape)\n",
    "    print(pred.shape)\n",
    "    print(pred[0].shape, pred[1].shape, pred[2].shape)\n",
    "# labels_dict = {\"0\": \"background\", \"1\": \"edema\", \"2\": \"non-enhancing tumor\", \"3\": \"enhancing tumour\"}\n",
    "\n",
    "# # The segementation volume contains values [0, 1, 2, 3]. As per BraTS summarizing paper for 2023, annotations comprise the \n",
    "# - 1 for NCR (necrotic tumor core)\n",
    "# - 2 for ED (peritumoral edematous/invaded tissue)\n",
    "# - 3 for ET (GD-enhancing tumor)\n",
    "# - 0 for everything else\n",
    "\n",
    "    enh = pred[2]\n",
    "    print(enh.shape)\n",
    "    pad = pred == 0.5\n",
    "    pred[pad==True] = 0\n",
    "\n",
    "    c1, c2, c3 = pred[0] > 0.4, pred[1] > 0.4, pred[2] > 0.375\n",
    "    print(\"CShapes: \", c1.shape, c2.shape, c3.shape)\n",
    "    # print(set(pred.flatten()))\n",
    "    print(\"before assignment:\" ,pred.shape)\n",
    "\n",
    "    # print(set(pred.flatten()))\n",
    "    pred = (c1 > 0).astype(np.uint8)\n",
    "    # print(set(pred.flatten()))\n",
    "        #  assigns 0 to elements in pred where c1 is False and 1 where c1 is True, effectively converting c1 to 0s and 1s in pred\n",
    "            ## c1 == NCR???\n",
    "\n",
    "    pred[(c2 == False) * (c1 == True)] = 2 #AA changed to 1\n",
    "    # print(set(pred.flatten()))\n",
    "        # This step assigns 2 to regions where c2 is False and c1 is True, which might indicate a region that extends beyond the threshold of c1 and is within the threshold of c2.\n",
    "            # if the WT probability for a given voxel is less than 0.45 then its class is set to 0 (background), otherwise if the probability for TC is less than 0.4 the voxel class is 2 (ED)\n",
    "\n",
    "    pred[(c3 == True) * (c1 == True)] = 3\n",
    "    # print(set(pred.flatten()))\n",
    "        # assigns 3 to regions where c3 is True and c1 is True, which might indicate a region that is within the threshold of both c1 and c3.\n",
    "        # finally if probability for ET is less than 0.4 voxel has class 1 (NCR), or otherwise 4 (ET).\n",
    "\n",
    "    print(\"after assignment: \", pred.shape)\n",
    "\n",
    "\n",
    "    components, n = label(pred == 3)\n",
    "    for et_idx in range(1, n + 1):\n",
    "        _, counts = np.unique(pred[components == et_idx], return_counts=True)\n",
    "        if 1 < counts[0] and counts[0] < 4 and np.mean(enh[components == et_idx]) < 0.9:\n",
    "            pred[components == et_idx] = 1\n",
    "    print(\"after for loop: \", pred.shape)\n",
    "    \n",
    "    et = pred == 3\n",
    "    if 0 < et.sum() and et.sum() < 5 and np.mean(enh[et]) < 0.9:\n",
    "        pred[et] = 1\n",
    "    print(\"before _lbl return: \", pred.shape)\n",
    "\n",
    "    pred = np.transpose(pred, (2, 1, 0)).astype(np.uint8)\n",
    "    print(\"after transpose: \", pred.shape)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def prepare_preditions(e):\n",
    "    fname = e.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    # fname = e.split(\"\\\\\")[-1].split(\".\")[0][:-6]\n",
    "    print(fname)\n",
    "    # CALL BELOW LINE FOR POST PROCESSING with ensembling\n",
    "    # preds = [np.load(f) for f in e]\n",
    "    # print(preds)\n",
    "    # p = to_lbl(np.mean(preds, 0))\n",
    "\n",
    "    # ELSE JUST SAVE NP MEANS ALONG AXIS 0\n",
    "    pred = np.load(e)\n",
    "    # p = np.load(e)\n",
    "\n",
    "    print(pred.shape)\n",
    "\n",
    "    p = to_lbl(pred)\n",
    "    print(p.shape)\n",
    "\n",
    "    return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing\n",
    "# Post-processing <a name=\"postprocessing\"></a>\n",
    "\n",
    "By optimizing the three overlapping regions (ET, TC, WT) we need to convert them back to the original classes (NCR, ED, ET). The strategy for transforming classes back to the original one is the following: if the WT probability for a given voxel is less than 0.45 then its class is set to 0 (background), otherwise if the probability for TC is less than 0.4 the voxel class is 2 (ED), and finally if probability for ET is less than 0.4 voxel has class 1 (NCR), or otherwise 4 (ET).\n",
    "\n",
    "Furthermore, we applied the following post-processing strategy: find ET connected components, for components smaller than 16 voxels with mean probability smaller than 0.9, replace their class to NCR (such that voxels are still considered part of the tumor core), next if there is overall less than 73 voxels with ET and their mean probability is smaller than 0.9 replace all ET voxels to NCR. With such post-processing we avoided the edge case where the model predicted a few voxels with enhancing tumor (ET) but there were not any in the ground truth. Such post-processing was beneficial to the final score as if there were no enhancing tumor voxels in the label, then the Dice score for zero false positive prediction was 1, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lbl(pred):\n",
    "        # pred = (3, 155, 240, 240)\n",
    "\n",
    "    enh = pred[2]\n",
    "\n",
    "    # labels_dict = {\"0\": \"background\", \"1\": \"edema\", \"2\": \"non-enhancing tumor\", \"3\": \"enhancing tumour\"}\n",
    "\n",
    "    c1, c2, c3 = pred[0] > 0.4, pred[1] > 0.35, pred[2] > 0.375\n",
    "    # (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
    "    #  ET               TC              WT\n",
    "\n",
    "    pred = (c1 > 0).astype(np.uint8)*2\n",
    "    # all prediction channels 0 or 1 --> assigning background or edema\n",
    "    pred[(c2 == False) * (c1 == True)] = 1\n",
    "    # something that is ET but not TC becomes NETC \n",
    "        \n",
    "    pred[(c3 == True) * (c1 == True)] = 3\n",
    "    # if WT is > .75, AND ET is > .4 --> assigned to enhancing tumour\n",
    "\n",
    "    components, n = label(pred == 3)\n",
    "    #  where predictions labelled 4, obtain the components and n\n",
    "    for et_idx in range(1, n + 1):\n",
    "        _, counts = np.unique(pred[components == et_idx], return_counts=True)\n",
    "        if 1 < counts[0] and counts[0] < 4 and np.mean(enh[components == et_idx]) < 0.9:\n",
    "            pred[components == et_idx] = 1\n",
    "            # obtain a count of enhancing tumour, if it is a a really small area (<4), and the m\n",
    "\n",
    "    et = pred == 4\n",
    "    if 0 < et.sum() and et.sum() < 5 and np.mean(enh[et]) < 0.9:\n",
    "        pred[et] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "### Saves initial prediction as .npy, and final prediction after label conversion as .nii.gz with tensor size 240, 240, 155\n",
    "\n",
    "**NOTE: MONAI_testfullrun appears to have used cropping of 192, 192, 128 --> NEED TO SEE WHEN AND WHY THIS WAS CHANGED**\n",
    "    AA HAS CORRECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dir=args.data\n",
    "validation_files = [os.path.join(validation_dir, file) for file in os.listdir(validation_dir)]\n",
    "print('\\n'.join(validation_files))\n",
    "\n",
    "model, n_channels = define_model(args.ckpt_path)\n",
    "\n",
    "# Load validation data to dataloader\n",
    "data_transforms = dl.define_transforms(n_channels)\n",
    "dataloader = dl.load_data(args, data_transforms)\n",
    "\n",
    "val_loader = dataloader['val']\n",
    "\n",
    "# Validation parameters\n",
    "VAL_AMP, dice_metric, dice_metric_batch, post_transforms = val_params()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "infer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing copied from OptiNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory exists\n",
      "[['C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=128-dice=93_74_task=01_fold=3_tta\\\\BraTS-SSA-00143-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=128-dice=93_74_task=01_fold=3_tta\\\\BraTS-SSA-00188-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=128-dice=93_74_task=01_fold=3_tta\\\\BraTS-SSA-00218-000.npy'], ['C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=21-dice=92_35_task=01_fold=2_tta\\\\BraTS-SSA-00192-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=21-dice=92_35_task=01_fold=2_tta\\\\BraTS-SSA-00210-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=21-dice=92_35_task=01_fold=2_tta\\\\BraTS-SSA-00227-000.npy'], ['C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=24-dice=89_04_task=01_fold=4_tta\\\\BraTS-SSA-00129-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=24-dice=89_04_task=01_fold=4_tta\\\\BraTS-SSA-00132-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=24-dice=89_04_task=01_fold=4_tta\\\\BraTS-SSA-00148-000.npy'], ['C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=48-dice=77_02_task=01_fold=0_tta\\\\BraTS-SSA-00126-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=48-dice=77_02_task=01_fold=0_tta\\\\BraTS-SSA-00169-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=48-dice=77_02_task=01_fold=0_tta\\\\BraTS-SSA-00198-000.npy'], ['C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=74-dice=90_31_task=01_fold=1_tta\\\\BraTS-SSA-00139-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=74-dice=90_31_task=01_fold=1_tta\\\\BraTS-SSA-00158-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=74-dice=90_31_task=01_fold=1_tta\\\\BraTS-SSA-00180-000.npy']]\n",
      "Preparing final predictions\n",
      "['C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=128-dice=93_74_task=01_fold=3_tta\\\\BraTS-SSA-00143-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=128-dice=93_74_task=01_fold=3_tta\\\\BraTS-SSA-00188-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=128-dice=93_74_task=01_fold=3_tta\\\\BraTS-SSA-00218-000.npy']\n",
      "BraTS-SSA-00143-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "BraTS-SSA-00188-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "BraTS-SSA-00218-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "['C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=21-dice=92_35_task=01_fold=2_tta\\\\BraTS-SSA-00192-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=21-dice=92_35_task=01_fold=2_tta\\\\BraTS-SSA-00210-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=21-dice=92_35_task=01_fold=2_tta\\\\BraTS-SSA-00227-000.npy']\n",
      "BraTS-SSA-00192-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "BraTS-SSA-00210-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "BraTS-SSA-00227-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "['C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=24-dice=89_04_task=01_fold=4_tta\\\\BraTS-SSA-00129-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=24-dice=89_04_task=01_fold=4_tta\\\\BraTS-SSA-00132-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=24-dice=89_04_task=01_fold=4_tta\\\\BraTS-SSA-00148-000.npy']\n",
      "BraTS-SSA-00129-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "BraTS-SSA-00132-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "BraTS-SSA-00148-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "['C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=48-dice=77_02_task=01_fold=0_tta\\\\BraTS-SSA-00126-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=48-dice=77_02_task=01_fold=0_tta\\\\BraTS-SSA-00169-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=48-dice=77_02_task=01_fold=0_tta\\\\BraTS-SSA-00198-000.npy']\n",
      "BraTS-SSA-00126-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "BraTS-SSA-00169-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "BraTS-SSA-00198-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "['C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=74-dice=90_31_task=01_fold=1_tta\\\\BraTS-SSA-00139-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=74-dice=90_31_task=01_fold=1_tta\\\\BraTS-SSA-00158-000.npy', 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=74-dice=90_31_task=01_fold=1_tta\\\\BraTS-SSA-00180-000.npy']\n",
      "BraTS-SSA-00139-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "BraTS-SSA-00158-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "BraTS-SSA-00180-000\n",
      "(3, 155, 240, 240)\n",
      "in to_lbl:  (3, 155, 240, 240)\n",
      "(3, 155, 240, 240)\n",
      "(155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "(155, 240, 240)\n",
      "CShapes:  (155, 240, 240) (155, 240, 240) (155, 240, 240)\n",
      "before assignment: (3, 155, 240, 240)\n",
      "after assignment:  (155, 240, 240)\n",
      "after for loop:  (155, 240, 240)\n",
      "before _lbl return:  (155, 240, 240)\n",
      "after transpose:  (240, 240, 155)\n",
      "(240, 240, 155)\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.makedirs(os.path.join(args.results, \"final_preds\"))\n",
    "except:\n",
    "    print(\"directory exists\")\n",
    "\n",
    "#--------------MONAI\n",
    "# examples = sorted(glob('C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\npy\\\\*'))\n",
    "orig_data = 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\Data\\\\SSA_Val'\n",
    "\n",
    "#------------- OPTINET     \n",
    "# orig_data='C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\images'\n",
    "preds = sorted(glob('C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions*'))\n",
    "# examples = list(zip(*[sorted(glob(f\"{p}\\\\*.npy\")) for p in preds]))\n",
    "examples = [sorted(glob(f\"{p}\\\\*.npy\")) for p in preds]\n",
    "print(examples)\n",
    "\n",
    "print(\"Preparing final predictions\")\n",
    "for e in examples:\n",
    "    print(e)\n",
    "    for pnpy in e:\n",
    "        pred = prepare_preditions(pnpy)\n",
    "        fname = pnpy.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        # # fname = e.split(\"\\\\\")[-1].split(\".\")[0][:-6]\n",
    "        # # print(pred.shape)\n",
    "        outfile = os.path.join(args.results, f\"{fname}.nii.gz\")\n",
    "        img = os.path.join(orig_data, fname, f\"{fname}-t1n.nii.gz\")\n",
    "        img_load = nib.load(img)\n",
    "        nib.save(\n",
    "            nib.Nifti1Image(pred, img_load.affine, header=img_load.header),\n",
    "        outfile\n",
    "        )\n",
    "    # save_final_preds(pred, img, outfile, fname)\n",
    "    \n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check saved nifty is of size 240, 240, 155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 240, 155)\n",
      "{0.0, 1.0, 2.0, 4.0}\n"
     ]
    }
   ],
   "source": [
    "sample = nib.load(os.path.join (args.results, \"BraTS-SSA-00132-000.nii.gz\"))\n",
    "print(sample.shape)\n",
    "\n",
    "print(set(np.array(sample.dataobj).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain background:  [2.26662227e-07 4.75793468e-09 8.99157735e-09]\n",
      "pad background:  [0.5 0.5 0.5]\n",
      "Yellow:  [1.17796997e-05 1.66149344e-08 7.68621096e-09]\n",
      "Green:  [9.99911574e-01 2.85174070e-06 7.06398265e-07]\n",
      "Red:  [9.99477039e-01 4.46913437e-05 3.46302622e-06]\n"
     ]
    }
   ],
   "source": [
    "lbl_chk = np.load('C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\results\\\\preds_allFolds\\\\predictions_epoch=128-dice=93_74_task=01_fold=3_tta\\\\BraTS-SSA-00143-000.npy')\n",
    "\n",
    "print(\"Brain background: \", lbl_chk[:,70,89,76])\n",
    "print(\"pad background: \",lbl_chk[:,70,227,216] )\n",
    "print(\"Yellow: \",lbl_chk[:,70,113,178])\n",
    "print(\"Green: \",lbl_chk[:,70,134,150])\n",
    "print(\"Red: \",lbl_chk[:,70,142,121])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise initial predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m max_idx \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(val_loader):\n\u001b[0;32m      3\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m<\u001b[39m max_idx:\n\u001b[0;32m      4\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_loader' is not defined"
     ]
    }
   ],
   "source": [
    "max_idx = 2\n",
    "for step, batch in enumerate(val_loader):\n",
    "    if step < max_idx:\n",
    "        input = batch[0]\n",
    "        print(input.shape)\n",
    "        print(str(batch[1]).split('/')[-1].replace(\"-stk.npy',)\", \"\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = 2\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(val_loader):\n",
    "        if step < max_idx:\n",
    "            input = batch[0]\n",
    "            case = str(batch[1]).split('/')[-1].replace(\"-stk.npy',)\", \"\")\n",
    "            # print(input.shape)\n",
    "            output = inference(VAL_AMP, model, input)\n",
    "            # print(output.shape)\n",
    "            val_output = post_transforms(output[0])\n",
    "            # print(val_output.shape)\n",
    "            \n",
    "            # visualize the 3 channels model output corresponding to this image\n",
    "            plt.figure(\"model output\", (18, 6))\n",
    "            for i in range(3):\n",
    "                plt.subplot(1, 3, i + 1)\n",
    "                plt.title(f\"{case}: output channel {i}\")\n",
    "                plt.imshow(val_output[i, :, :, 75])\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m data \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(glob(\u001b[39m\"\u001b[39m\u001b[39m/scratch/guest187/Data/val_SSA/results/monai_test/*.nii.gz\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[1;32m----> 9\u001b[0m     fname \u001b[39m=\u001b[39m data[i]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(fname)\n\u001b[0;32m     11\u001b[0m     img \u001b[39m=\u001b[39m nib\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/scratch/guest187/Data/val_SSA/finetune_ssa/images/\u001b[39m\u001b[39m{\u001b[39;00mfname\u001b[39m}\u001b[39;00m\u001b[39m-stk.nii.gz\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mget_fdata()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "\n",
    "n, z = 5, 75\n",
    "data = sorted(glob(\"/scratch/guest187/Data/val_SSA/results/monai_test/*.nii.gz\"))\n",
    "for i in range(n):\n",
    "    fname = data[i].split(\"/\")[-1].split(\".\")[0]\n",
    "    print(fname)\n",
    "    img = nib.load(f\"/scratch/guest187/Data/val_SSA/finetune_ssa/images/{fname}-stk.nii.gz\").get_fdata().astype(np.float32)\n",
    "    pred = nib.load(data[i]).get_fdata().astype(np.uint8)[:, :, z]\n",
    "    imgs = [img[:, :, z, i] for i in [0, 3]] + [pred]\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 12))\n",
    "    for i in range(3):\n",
    "        if i < 2:\n",
    "            ax[i].imshow(imgs[i], cmap='gray')\n",
    "        else:\n",
    "            ax[i].imshow(imgs[i]);\n",
    "        ax[i].axis('off')  \n",
    "    plt.tight_layout()            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\UNN_BraTS23\\\\scripts')\n",
    "import subprocess\n",
    "try:\n",
    "    import os\n",
    "    import numpy as np \n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    import nibabel as nib\n",
    "    import cv2\n",
    "    from skimage.transform import resize\n",
    "except ModuleNotFoundError as e:\n",
    "    package = str(e).split(\"'\")[0]\n",
    "    subprocess.run(['pip', 'install', package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pth = 'C:\\\\Users\\\\amoda\\\\Documents\\\\SPARK\\\\BraTS2023\\\\CC\\\\Backup_2407\\\\val_SSA\\\\results\\\\monai_test2\\\\npy'\n",
    "pred_files = [os.path.join(pred_pth, file) for file in os.listdir(pred_pth)]\n",
    "# Calculate the amount of padding needed\n",
    "desired_height = 240\n",
    "desired_width = 240\n",
    "desired_depth = 155\n",
    "\n",
    "\n",
    "for file in pred_files:\n",
    "    pred1 = nib.load(file)\n",
    "    image = pred1.get_fdata().astype(np.uint8)\n",
    "    print(pred.shape)\n",
    "    padding_height = max(0, desired_height - image.shape[0])\n",
    "    padding_width = max(0, desired_width - image.shape[1])\n",
    "    padding_depth = max(0, desired_depth - image.shape[2])\n",
    "  \n",
    "    # Add padding to the image\n",
    "    padded_image = cv2.copyMakeBorder(image, 0, padding_height, 0, padding_width, 0, padding_depth, 0, cv2.BORDER_CONSTANT)\n",
    "\n",
    "    print(padded_image.shape)\n",
    "\n",
    "    # Now 'padded_image' will contain the original image with padding added to achieve the desired dimensions\n",
    "    # Create a new nibabel image with the resized data\n",
    "    resized_image = nib.Nifti1Image(padded_image, pred1.affine, pred1.header)\n",
    "    print(resized_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(checkpoint=None):\n",
    "    model=UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        # channels=(32, 64, 128, 256, 320, 320), #nnunet channels, depth 6\n",
    "        # channels=(64, 96, 128, 192, 256, 384, 512), # optinet, depth 7\n",
    "        strides=(2, 2, 2, 2), # length should = len(channels) - 1\n",
    "        # kernel_size=,\n",
    "        # num_res_units=,\n",
    "        # dropout=0.0,\n",
    "        )\n",
    "    n_channels = len(model.channels)\n",
    "    print(f\"Number of channels: {n_channels}\")\n",
    "\n",
    "    if checkpoint != None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=torch.device('cpu')))\n",
    "\n",
    "    return model, n_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pth = '/scratch/guest187/Data/train_gli_hack/results_hack_finetune_ssa/checkpoints/f0_finetune/epoch=39-dice=91.38.ckpt'\n",
    "# pth = '/scratch/guest187/Data/train_all/results/test_fullRunThrough/best_metric_model_fullTest.pth'\n",
    "pth = args.ckpt_path\n",
    "checkpoint = torch.load(pth, map_location=torch.device('cpu'))\n",
    "keys = checkpoint.keys()\n",
    "\n",
    "# for key in checkpoint.keys():\n",
    "#     print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import torchio as tio\n",
    "import logging\n",
    "\n",
    "# import torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Import MONAI libraries                <--- CLEAN UP THESE IMPORTS ONCE WE KNOW WHAT libraries are used\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "from monai.data import ArrayDataset, decollate_batch, DataLoader\n",
    "from monai.handlers import (\n",
    "    CheckpointLoader,\n",
    "    IgniteMetric,\n",
    "    MeanDice,\n",
    "    StatsHandler,\n",
    "    TensorBoardImageHandler,\n",
    "    TensorBoardStatsHandler,\n",
    ")\n",
    "from monai.metrics import DiceMetric, LossMetric, HausdorffDistanceMetric\n",
    "from monai.losses import DiceLoss, DiceFocalLoss\n",
    "from monai.networks import nets as monNets\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirst,\n",
    "    AsDiscrete,\n",
    "    Compose\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.utils import first\n",
    "from monai.utils.misc import set_determinism\n",
    "\n",
    "# Other imports (unsure)\n",
    "# import ignite\n",
    "import nibabel\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import torch.utils.data as data_utils\n",
    "import json\n",
    "from subprocess import call\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from utils.utils import get_main_args\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Given a set of images and corresponding labels (i.e, will give it all training images + labels, and same for val and test)\n",
    "    folder structure: subjectID/subjectID-stk.npy, -lbl.npy (i.e. contains 2 files)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, data_folders, transform=None, SSAtransform=None):\n",
    "            self.data_folders = data_folders                            # path for each data folder in the set\n",
    "            self.transform = transform\n",
    "            self.SSAtransform = SSAtransform\n",
    "            self.imgs = []                                              # store images to load (paths)\n",
    "            self.lbls = []                                              # store corresponding labels (paths)\n",
    "            for img_folder in self.data_folders:                        # run through each subjectID folder\n",
    "                folder_path = os.path.join(data_dir, img_folder)                                                            \n",
    "                self.SSA = True if 'SSA' in img_folder else False       # check if current file is from SSA dataset\n",
    "                for file in os.listdir(folder_path):                    # check folder contents\n",
    "                    if os.path.isfile(os.path.join(folder_path, file)):\n",
    "                        if file.endswith(\"-lbl.npy\"):\n",
    "                            self.lbls.append(os.path.join(folder_path, file))   # Save segmentation mask (file path)\n",
    "                            self.mode = \"labels\"\n",
    "                        elif file.endswith(\"-stk.npy\"):\n",
    "                            self.imgs.append(os.path.join(folder_path, file))   # Save image (file path)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the amount of images in this set\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        name = os.path.dirname(self.imgs[idx])\n",
    "        # Load files\n",
    "        if self.mode == \"labels\":\n",
    "            mask = np.load(self.lbls[idx])\n",
    "            mask = torch.from_numpy(mask) # 240, 240, 155\n",
    "\n",
    "        # print(self.imgs[idx] )\n",
    "        # print(\"========================\")\n",
    "        # print(self.lbls[idx] )\n",
    "        # print(\"========================\")           \n",
    "\n",
    "        if self.transform is not None: # Apply general transformations\n",
    "        # transforms such as crop, flip, rotate etc will be applied to both the image and the mask\n",
    "            if self.mode == \"labels\":\n",
    "                subject = tio.Subject(\n",
    "                    image=tio.ScalarImage(tensor=image),\n",
    "                    mask=tio.LabelMap(tensor=mask)\n",
    "                    )\n",
    "                tranformed_subject = self.transform(subject)\n",
    "                # Apply transformation to GLI data to reduce quality (creating fake SSA data)\n",
    "                if self.SSA == False and self.SSAtransform is not None:\n",
    "                    tranformed_subject = self.SSAtransform(tranformed_subject)\n",
    "            \n",
    "                print(\"Tranformed_subject: \", tranformed_subject)\n",
    "                image = tranformed_subject[\"image\"].data\n",
    "                mask = tranformed_subject[\"mask\"].data\n",
    "                return image, mask, self.imgs[idx]\n",
    "            else:\n",
    "                subject = tio.Subject(\n",
    "                    image=tio.ScalarImage(tensor=image),\n",
    "                    )\n",
    "                tranformed_subject = self.transform(subject)           \n",
    "                print(\"Tranformed_subject: \", tranformed_subject)\n",
    "                image = tranformed_subject[\"image\"].data\n",
    "                return image, self.imgs[idx]\n",
    "\n",
    "        return image, mask, self.imgs[idx]\n",
    "    \n",
    "    def get_paths(self):\n",
    "        return self.img_pth, self.seg_pth\n",
    "    \n",
    "    def get_subj_info(self):\n",
    "        return self.subj_dir_pths, self.subj_dirs\n",
    "        #, self.SSA\n",
    "    \n",
    "    def get_transforms(self):\n",
    "        return self.transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_transforms(n_channels):\n",
    "    # Initialise data transforms\n",
    "    data_transforms = {\n",
    "        'train': tio.Compose([\n",
    "            tio.CropOrPad((192, 192, 124)),\n",
    "            tio.OneOf([\n",
    "                tio.Compose([\n",
    "                    tio.RandomFlip(axes=0, p=0.3),\n",
    "                    tio.RandomFlip(axes=1, p=0.3),\n",
    "                    tio.RandomFlip(axes=2, p=0.3)]),\n",
    "                tio.RandomAffine(degrees=15,p=0.3)\n",
    "            ], p=0.8),\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ]),\n",
    "        'fakeSSA': tio.OneOf({\n",
    "            tio.OneOf({\n",
    "                tio.Compose([\n",
    "                    tio.Resample((1.2, 1.2, 6), scalars_only=True),\n",
    "                    tio.Resample(1)\n",
    "                ]):0.50,\n",
    "                tio.Compose([\n",
    "                    tio.RandomAnisotropy(axes=(1, 2), downsampling=(1.2), scalars_only=True),\n",
    "                    tio.RandomAnisotropy(axes=0, downsampling=(6), scalars_only=True)\n",
    "                ]):0.5,                \n",
    "            },p=0.80),\n",
    "            tio.Compose([            \n",
    "                tio.OneOf({\n",
    "                    tio.RandomBlur(std=(0.5, 1.5)) : 0.3,\n",
    "                    tio.RandomNoise(mean=3, std=(0, 0.33)) : 0.7\n",
    "                },p=0.50),\n",
    "                tio.OneOf({\n",
    "                    tio.RandomMotion(num_transforms=3, image_interpolation='nearest') : 0.5,\n",
    "                    tio.RandomBiasField(coefficients=1) : 0.2,\n",
    "                    tio.RandomGhosting(intensity=1.5) : 0.3\n",
    "                }, p=0.50)\n",
    "            ])\n",
    "        }, p=0.8), # randomly apply ONE of these given transforms with prob 0.5 \n",
    "        'val': tio.Compose([\n",
    "            tio.CropOrPad((192, 192, 124)),\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ]),\n",
    "        'test' : tio.Compose([\n",
    "            tio.EnsureShapeMultiple(2**n_channels, method='pad')\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    return data_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args, data_transforms):\n",
    "\n",
    "    '''\n",
    "    This function is called during training after define_transforms(n_channels)\n",
    "\n",
    "    It takes as input\n",
    "        args: argparsers from the utils script \n",
    "            args.seed\n",
    "            args.data_used: 'all', 'GLI', 'SSA'\n",
    "        data_transforms: a dictionary of transformations to apply to the data during training\n",
    "\n",
    "    Returns dataloaders ready to be fed into model\n",
    "    '''\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Set a seed for reproducibility if you want the same split - optional\n",
    "    if args.seed != None:\n",
    "        seed=args.seed\n",
    "        logger.info(f\"Seed set to {seed}.\")\n",
    "    else:\n",
    "        seed=None\n",
    "        logger.info(\"No seed has been set\")\n",
    "    \n",
    "    # Locate data based on which dataset is being used\n",
    "    if args.data_used == 'all':\n",
    "        data_folders = glob.glob(os.path.join(args.data, \"BraTS*\"))\n",
    "    elif args.data_used == \"GLI\":\n",
    "        data_folders = [folder for folder in os.listdir(args.data) if 'GLI' in folder]\n",
    "    elif args.data_used == 'SSA':\n",
    "        data_folders = [folder for folder in os.listdir(args.data) if 'SSA' in folder]\n",
    "\n",
    "    # Split data files\n",
    "    train_files, val_files = split_data(data_folders, seed) \n",
    "    logger.info(f\"Number of training files: {len(train_files)}\\nNumber of validation files: {len(val_files)}\")\n",
    "    \n",
    "    image_datasets = {\n",
    "        'train': MRIDataset(args.data, train_files, transform=data_transforms['train']),\n",
    "        'val': MRIDataset(args.data, val_files, transform=data_transforms['val']),\n",
    "    }\n",
    "\n",
    "    # Create dataloaders\n",
    "    # can set num_workers for running sub-processes\n",
    "    dataloaders = {\n",
    "        'train': data_utils.DataLoader(image_datasets['train'], batch_size=args.batch_size, shuffle=True, drop_last=True),\n",
    "        'val': data_utils.DataLoader(image_datasets['val'], batch_size=args.val_batch_size, shuffle=True)\n",
    "        # 'test': data_utils.DataLoader(image_datasets['test'], batch_size=args.val_batch_size, shuffle=True)\n",
    "    }\n",
    "\n",
    "    # Save data split\n",
    "    splitData = {\n",
    "        'subjsTr' : train_files,\n",
    "        'subjsVal' : val_files,\n",
    "        # 'subjsTest' : test_files    \n",
    "    }\n",
    "    with open(args.data + str(args.data_used) + \".json\", \"w\") as file:\n",
    "        json.dump(splitData, file)\n",
    "\n",
    "    return dataloaders\n",
    "\n",
    "def split_data(data_folders, seed):\n",
    "    '''\n",
    "    Function to split dataset into train/val/test splits, given all avilable data.\n",
    "    Input:\n",
    "        list of paths to numpy files\n",
    "    Returns:\n",
    "        lists for each train and val/test sets, where each list contains the file names to be used in the set\n",
    "    '''\n",
    "    #-----------------------------\n",
    "    # originally we split as 3: train-test-val train (70), val (15), test (15):\n",
    "        # train_files, test_files = train_test_split(data_folders, test_size=0.7, random_state=seed)\n",
    "        # val_files, test_files = train_test_split(test_files, test_size=0.5, random_state=seed)\n",
    "\n",
    "    #-----------------------------\n",
    "    # training loop split is train-val (70-30)\n",
    "    train_files, val_files = train_test_split(data_folders, test_size=0.7, random_state=seed)\n",
    "\n",
    "    # ??? validation/testing???\n",
    "\n",
    "    return train_files, val_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup transforms, dataset\"\"\"\n",
    "def define_dataloaders(n_channels):\n",
    "    # Define transforms\n",
    "    data_transform = define_transforms(n_channels)\n",
    "    # Load data\n",
    "    dataloaders = load_data(args, data_transform)                      # this also saves a json splitData\n",
    "    # train_loader, val_loader = dataloaders['train'], dataloaders['val']\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define model architecture:\n",
    "        Done before data loader so that transforms has n_channels for EnsureShapeMultiple\n",
    "\"\"\"\n",
    "def define_model(checkpoint=None):\n",
    "    model=UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        # channels=(32, 64, 128, 256, 320, 320), #nnunet channels, depth 6\n",
    "        # channels=(64, 96, 128, 192, 256, 384, 512), # optinet, depth 7\n",
    "        strides=(2, 2, 2, 2), # length should = len(channels) - 1\n",
    "        # kernel_size=,\n",
    "        # num_res_units=,\n",
    "        # dropout=0.0,\n",
    "        )\n",
    "    n_channels = len(model.channels)\n",
    "    print(f\"Number of channels: {n_channels}\")\n",
    "\n",
    "    if checkpoint != None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=torch.device('cpu')))\n",
    "\n",
    "    return model, n_channels\n",
    "\n",
    "\"\"\"\n",
    "Setup validation stuff\n",
    "    metrics\n",
    "    post trans ???????\n",
    "    define inference\n",
    "\"\"\"\n",
    "def val_params():\n",
    "    VAL_AMP = True\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=True, num_classes=4)\n",
    "    dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\", get_not_nans=True, num_classes=4)\n",
    "    post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "    return VAL_AMP, dice_metric, dice_metric_batch, post_trans\n",
    "\n",
    "# define inference method\n",
    "def inference(VAL_AMP, model, input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=None, ## best to leave as None, computes itself (else NEEDS TO BE BASED ON LAYER IN, OUT E.G., Conv3d(4, 16; Conv3d(16, 32; etc)\n",
    "            # roi_size=(128, 128, 64),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "            mode='gaussian'\n",
    "        )\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"General Setup: \n",
    "    logging,\n",
    "    utils.args \n",
    "    seed,\n",
    "    cuda, \n",
    "    root dir\"\"\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# args = get_main_args()\n",
    "seed = 42\n",
    "set_determinism(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# root_dir = args.data\n",
    "# results_dir = args.results\n",
    "\n",
    "model, n_channels = define_model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File paths for Alex's local system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dir= '/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/val_SSA'\n",
    "validation_files = [os.path.join(validation_dir, file) for file in os.listdir(validation_dir)]\n",
    "\n",
    "# checkpoint to test: /scratch/guest187/Data/train_all/results/test_run/best_metric_model.pth \n",
    "checkpoint = '/Users/alexandrasmith/Desktop/Workspace/Projects/UNN_BraTS23/data/best_metric_model_fullTest.pth'\n",
    "\n",
    "# Load validation data to dataloader\n",
    "data_transforms = define_transforms(n_channels)\n",
    "validation_dataset = MRIDataset(validation_dir, validation_files, transform=data_transforms['val'])\n",
    "\n",
    "# Validation parameters\n",
    "VAL_AMP, dice_metric, dice_metric_batch, post_transforms = val_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(validation_dataset))\n",
    "img, image_path = validation_dataset[0]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, n_channels = define_model(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_input, _ = validation_dataset[0]\n",
    "    print(val_input.shape)\n",
    "    # roi_size = (128, 128, 64)\n",
    "    # sw_batch_size = 4\n",
    "    val_output = inference(VAL_AMP, model, val_input)\n",
    "    val_output = post_transforms(val_output[0])\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(validation_dataset[6][\"image\"][i, :, :, 70].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(validation_dataset[6][\"label\"][i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
